{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§  Lecture 3: From First Principles to Production Pipeline\n",
        "\n",
        "## ðŸŽ¯ Learning Objectives\n",
        "1. **Experience** raw data loading from URLs (no libraries)\n",
        "2. **Stress test** DNNs across 3 architectures Ã— 3 datasets (9 models)\n",
        "3. **Visualize** training dynamics with comprehensive plots\n",
        "4. **Design** production-ready ML pipeline architecture\n",
        "5. **Deploy** interactive UI for model comparison\n",
        "\n",
        "### ðŸ§­ Experiment Matrix: 3Ã—3 Grid\n",
        "| Architecture \\ Dataset | MNIST | Fashion MNIST | CIFAR-10 |\n",
        "|------------------------|-------|---------------|----------|\n",
        "| **Simple DNN** (1 hidden) | Model 1 | Model 2 | Model 3 |\n",
        "| **Medium DNN** (2 hidden) | Model 4 | Model 5 | Model 6 |\n",
        "| **Deep DNN** (3 hidden) | Model 7 | Model 8 | Model 9 |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ PART 1: Industrial-Grade Data Loading System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "import os\n",
        "import gzip\n",
        "import struct\n",
        "import tarfile\n",
        "import pickle as pkl\n",
        "import urllib.request\n",
        "from urllib.error import URLError, HTTPError\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Pipeline directory structure\n",
        "BASE_DIR = Path(\"./dl_pipeline_lecture3\")\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "MODELS_DIR = BASE_DIR / \"models\"\n",
        "RESULTS_DIR = BASE_DIR / \"results\"\n",
        "VIZ_DIR = BASE_DIR / \"visualizations\"\n",
        "\n",
        "for dir_path in [BASE_DIR, DATA_DIR, MODELS_DIR, RESULTS_DIR, VIZ_DIR]:\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"ðŸ—ï¸  Pipeline structure created\")\n",
        "print(f\"   Data: {DATA_DIR}\")\n",
        "print(f\"   Models: {MODELS_DIR}\")\n",
        "print(f\"   Results: {RESULTS_DIR}\")\n",
        "print(f\"   Visualizations: {VIZ_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# -------------------------------------------------------------------\n",
        "# ðŸ“Œ DATASETS with multiple mirrors for reliability\n",
        "# -------------------------------------------------------------------\n",
        "DATASETS = {\n",
        "    \"mnist\": {\n",
        "        \"train_images\": [\n",
        "            \"https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz\",\n",
        "            \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
        "        ],\n",
        "        \"train_labels\": [\n",
        "            \"https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz\",\n",
        "            \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
        "        ],\n",
        "        \"test_images\": [\n",
        "            \"https://storage.googleapis.com/cvdf-datasets/mnist/t10k-images-idx3-ubyte.gz\",\n",
        "            \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
        "        ],\n",
        "        \"test_labels\": [\n",
        "            \"https://storage.googleapis.com/cvdf-datasets/mnist/t10k-labels-idx1-ubyte.gz\",\n",
        "            \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",\n",
        "        ],\n",
        "    },\n",
        "    \"fashion\": {\n",
        "        \"train_images\": [\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\"],\n",
        "        \"train_labels\": [\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\"],\n",
        "        \"test_images\": [\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\"],\n",
        "        \"test_labels\": [\"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\"],\n",
        "    },\n",
        "    \"cifar10\": {\n",
        "        \"train_batch\": [\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"],\n",
        "    }\n",
        "}\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# ðŸ“Œ Download with retries and progress bar\n",
        "# -------------------------------------------------------------------\n",
        "def download_with_retry(url_list, out_path, retries=3):\n",
        "    \"\"\"Robust download with progress bar and retry logic\"\"\"\n",
        "    for url in url_list:\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                print(f\"   Downloading: {os.path.basename(out_path)} (attempt {attempt+1}/{retries})\")\n",
        "                \n",
        "                with urllib.request.urlopen(url) as response:\n",
        "                    total = int(response.headers.get(\"Content-Length\", 0))\n",
        "                    with open(out_path, \"wb\") as f, tqdm(\n",
        "                        total=total, unit=\"B\", unit_scale=True, \n",
        "                        desc=os.path.basename(out_path)[:30]\n",
        "                    ) as bar:\n",
        "                        while True:\n",
        "                            chunk = response.read(8192)\n",
        "                            if not chunk:\n",
        "                                break\n",
        "                            f.write(chunk)\n",
        "                            bar.update(len(chunk))\n",
        "                \n",
        "                print(\"     âœ“ Download successful\")\n",
        "                return True\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"     âš  Error: {e}\\n     Retrying...\")\n",
        "    \n",
        "    print(f\"     âŒ Failed to download: {url_list}\")\n",
        "    return False\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# ðŸ“Œ Parse CIFAR-10 files\n",
        "# -------------------------------------------------------------------\n",
        "def load_cifar10_batch(file_path):\n",
        "    \"\"\"Load single CIFAR-10 batch\"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        batch = pkl.load(f, encoding='latin1')\n",
        "        X = batch['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "        y = np.array(batch['labels'])\n",
        "    return X, y\n",
        "\n",
        "def load_cifar10(root):\n",
        "    \"\"\"Load all CIFAR-10 batches\"\"\"\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    \n",
        "    # Load training batches\n",
        "    for i in range(1, 6):\n",
        "        batch_path = os.path.join(root, f\"data_batch_{i}\")\n",
        "        X_batch, y_batch = load_cifar10_batch(batch_path)\n",
        "        X_train.append(X_batch)\n",
        "        y_train.append(y_batch)\n",
        "    \n",
        "    # Concatenate\n",
        "    X_train = np.concatenate(X_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "    \n",
        "    # Load test batch\n",
        "    test_path = os.path.join(root, \"test_batch\")\n",
        "    X_test, y_test = load_cifar10_batch(test_path)\n",
        "    \n",
        "    # CIFAR-10 classes\n",
        "    cifar10_classes = [\n",
        "        'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "        'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "    ]\n",
        "    \n",
        "    return (X_train, y_train), (X_test, y_test), cifar10_classes\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# ðŸ“Œ Industrial Data Loader\n",
        "# -------------------------------------------------------------------\n",
        "class IndustrialDataLoader:\n",
        "    \"\"\"Load datasets directly from URLs with caching\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset=\"mnist\"):\n",
        "        assert dataset in DATASETS, f\"Unknown dataset. Available: {list(DATASETS.keys())}\"\n",
        "        self.dataset = dataset\n",
        "        self.root = DATA_DIR / dataset\n",
        "        self.root.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Class names for each dataset\n",
        "        if dataset == \"mnist\":\n",
        "            self.classes = [str(i) for i in range(10)]\n",
        "        elif dataset == \"fashion\":\n",
        "            self.classes = [\n",
        "                \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "                \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
        "            ]\n",
        "        elif dataset == \"cifar10\":\n",
        "            self.classes = [\n",
        "                'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "            ]\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Load dataset with progress indication\"\"\"\n",
        "        print(f\"ðŸ“¥ Loading {self.dataset.upper()} from source URLs...\")\n",
        "        \n",
        "        if self.dataset in [\"mnist\", \"fashion\"]:\n",
        "            return self._load_mnist_style()\n",
        "        else:  # cifar10\n",
        "            return self._load_cifar10()\n",
        "    \n",
        "    def _load_mnist_style(self):\n",
        "        \"\"\"Load MNIST or Fashion MNIST\"\"\"\n",
        "        # Download files\n",
        "        for file_type, urls in DATASETS[self.dataset].items():\n",
        "            file_path = self.root / f\"{file_type}.gz\"\n",
        "            if not file_path.exists():\n",
        "                download_with_retry(urls, file_path)\n",
        "        \n",
        "        # Parse IDX format\n",
        "        def load_idx_images(filename):\n",
        "            with gzip.open(filename, 'rb') as f:\n",
        "                magic, num, rows, cols = struct.unpack('>IIII', f.read(16))\n",
        "                images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num, rows, cols)\n",
        "            return images\n",
        "        \n",
        "        def load_idx_labels(filename):\n",
        "            with gzip.open(filename, 'rb') as f:\n",
        "                magic, num = struct.unpack('>II', f.read(8))\n",
        "                labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "            return labels\n",
        "        \n",
        "        # Load data\n",
        "        X_train = load_idx_images(self.root / \"train_images.gz\")\n",
        "        y_train = load_idx_labels(self.root / \"train_labels.gz\")\n",
        "        X_test = load_idx_images(self.root / \"test_images.gz\")\n",
        "        y_test = load_idx_labels(self.root / \"test_labels.gz\")\n",
        "        \n",
        "        print(f\"     âœ“ {self.dataset.upper()}: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
        "        return (X_train, y_train), (X_test, y_test), self.classes\n",
        "    \n",
        "    def _load_cifar10(self):\n",
        "        \"\"\"Load CIFAR-10\"\"\"\n",
        "        # Download and extract\n",
        "        tar_path = self.root / \"cifar-10-python.tar.gz\"\n",
        "        extracted_path = self.root / \"cifar-10-batches-py\"\n",
        "        \n",
        "        if not tar_path.exists():\n",
        "            download_with_retry(DATASETS[\"cifar10\"][\"train_batch\"], tar_path)\n",
        "        \n",
        "        if not extracted_path.exists():\n",
        "            print(\"     Extracting CIFAR-10...\")\n",
        "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "                tar.extractall(path=self.root)\n",
        "        \n",
        "        # Load data\n",
        "        (X_train, y_train), (X_test, y_test), _ = load_cifar10(extracted_path)\n",
        "        \n",
        "        print(f\"     âœ“ CIFAR-10: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
        "        return (X_train, y_train), (X_test, y_test), self.classes\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# ðŸ“Œ Load and visualize all datasets\n",
        "# -------------------------------------------------------------------\n",
        "print(\"\\nðŸ“Š LOADING ALL 3 DATASETS FROM SOURCE URLs\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_datasets = {}\n",
        "for dataset_name in [\"mnist\", \"fashion\", \"cifar10\"]:\n",
        "    loader = IndustrialDataLoader(dataset=dataset_name)\n",
        "    train_data, test_data, classes = loader.load()\n",
        "    all_datasets[dataset_name] = {\n",
        "        'train': train_data,\n",
        "        'test': test_data,\n",
        "        'classes': classes\n",
        "    }\n",
        "\n",
        "# Visualize dataset complexity\n",
        "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
        "\n",
        "for row_idx, (dataset_name, data_info) in enumerate(all_datasets.items()):\n",
        "    X_train, y_train = data_info['train']\n",
        "    classes = data_info['classes']\n",
        "    \n",
        "    # Show 5 random samples per dataset\n",
        "    indices = np.random.choice(len(X_train), 5, replace=False)\n",
        "    \n",
        "    for col_idx, idx in enumerate(indices):\n",
        "        ax = axes[row_idx, col_idx]\n",
        "        img = X_train[idx]\n",
        "        \n",
        "        if dataset_name == \"cifar10\":\n",
        "            ax.imshow(img.astype(np.uint8))\n",
        "        else:\n",
        "            ax.imshow(img, cmap='gray')\n",
        "        \n",
        "        label = y_train[idx]\n",
        "        ax.set_title(f\"{classes[label][:10]}\", fontsize=9)\n",
        "        ax.axis('off')\n",
        "    \n",
        "    # Dataset title\n",
        "    axes[row_idx, 2].text(0.5, 1.2, f\"{dataset_name.upper()}\\n{X_train.shape[1:]} {X_train.dtype}\", \n",
        "                          ha='center', va='center', transform=axes[row_idx, 2].transAxes,\n",
        "                          fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Dataset Complexity Gradient: MNIST â†’ Fashion MNIST â†’ CIFAR-10', \n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(VIZ_DIR / \"dataset_complexity.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ PART 2: Modular Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "class DataPreprocessor:\n",
        "    \"\"\"Modular preprocessing with configurable transformations\"\"\"\n",
        "    \n",
        "    def __init__(self, flatten=True, normalize=True, rgb_to_grayscale=False):\n",
        "        self.flatten = flatten\n",
        "        self.normalize = normalize\n",
        "        self.rgb_to_grayscale = rgb_to_grayscale\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Apply preprocessing pipeline\"\"\"\n",
        "        X = X.copy().astype(np.float32)\n",
        "        \n",
        "        # 1. Convert RGB to grayscale if needed (for DNN comparison)\n",
        "        if self.rgb_to_grayscale and X.ndim == 4 and X.shape[-1] == 3:\n",
        "            X = np.mean(X, axis=-1, keepdims=True)\n",
        "            \n",
        "        # 2. Normalize to [0, 1]\n",
        "        if self.normalize:\n",
        "            X = X / 255.0\n",
        "            \n",
        "        # 3. Flatten for DNN\n",
        "        if self.flatten and X.ndim > 2:\n",
        "            original_shape = X.shape\n",
        "            X = X.reshape(X.shape[0], -1)\n",
        "            \n",
        "        # 4. One-hot encode labels if provided\n",
        "        if y is not None:\n",
        "            y = y.astype(int)\n",
        "            n_classes = len(np.unique(y))\n",
        "            y_onehot = np.zeros((len(y), n_classes))\n",
        "            y_onehot[np.arange(len(y)), y] = 1\n",
        "            return X, y_onehot\n",
        "        \n",
        "        return X\n",
        "    \n",
        "    def prepare_dataset(self, X_train, y_train, X_test, y_test, val_size=0.1):\n",
        "        \"\"\"Complete dataset preparation\"\"\"\n",
        "        # Transform\n",
        "        X_train_proc, y_train_proc = self.transform(X_train, y_train)\n",
        "        X_test_proc, y_test_proc = self.transform(X_test, y_test)\n",
        "        \n",
        "        # Create validation split\n",
        "        X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "            X_train_proc, y_train_proc, \n",
        "            test_size=val_size, \n",
        "            random_state=42,\n",
        "            stratify=y_train\n",
        "        )\n",
        "        \n",
        "        print(f\"   Prepared: {X_train.shape} â†’ {X_train_final.shape} (train)\")\n",
        "        print(f\"             {X_test.shape} â†’ {X_test_proc.shape} (test)\")\n",
        "        print(f\"             {X_val.shape} (validation)\")\n",
        "        \n",
        "        return {\n",
        "            'X_train': X_train_final, 'y_train': y_train_final,\n",
        "            'X_val': X_val, 'y_val': y_val,\n",
        "            'X_test': X_test_proc, 'y_test': y_test_proc\n",
        "        }\n",
        "\n",
        "# Prepare all datasets\n",
        "print(\"\\nðŸ”§ PREPROCESSING ALL DATASETS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "prepared_data = {}\n",
        "for dataset_name, data_info in all_datasets.items():\n",
        "    print(f\"\\nProcessing {dataset_name.upper()}:\")\n",
        "    \n",
        "    X_train, y_train = data_info['train']\n",
        "    X_test, y_test = data_info['test']\n",
        "    \n",
        "    # Special preprocessing for CIFAR-10\n",
        "    if dataset_name == \"cifar10\":\n",
        "        preprocessor = DataPreprocessor(flatten=True, rgb_to_grayscale=True)\n",
        "    else:\n",
        "        preprocessor = DataPreprocessor(flatten=True)\n",
        "    \n",
        "    data_dict = preprocessor.prepare_dataset(X_train, y_train, X_test, y_test)\n",
        "    prepared_data[dataset_name] = {\n",
        "        'data': data_dict,\n",
        "        'classes': data_info['classes'],\n",
        "        'preprocessor': preprocessor,\n",
        "        'original_shape': X_train.shape[1:]\n",
        "    }\n",
        "\n",
        "print(\"\\nâœ… All datasets preprocessed and ready for training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ—ï¸ PART 3: Neural Network Components (First Principles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "import pickle\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "class NeuralMath:\n",
        "    \"\"\"Pure mathematical operations - no state\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        return np.maximum(0, x)\n",
        "    \n",
        "    @staticmethod\n",
        "    def relu_derivative(x):\n",
        "        return (x > 0).astype(np.float32)\n",
        "    \n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "    \n",
        "    @staticmethod\n",
        "    def cross_entropy(y_pred, y_true):\n",
        "        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
        "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "    \n",
        "    @staticmethod\n",
        "    def cross_entropy_gradient(y_pred, y_true):\n",
        "        return y_pred - y_true\n",
        "    \n",
        "    @staticmethod\n",
        "    def initialize_weights(shape, activation='relu'):\n",
        "        \"\"\"Smart initialization\"\"\"\n",
        "        if activation == 'relu':\n",
        "            # He initialization\n",
        "            std = np.sqrt(2.0 / shape[0])\n",
        "        else:\n",
        "            # Xavier initialization\n",
        "            std = np.sqrt(2.0 / (shape[0] + shape[1]))\n",
        "        return np.random.randn(*shape) * std\n",
        "\n",
        "class DenseLayer:\n",
        "    \"\"\"Single neural network layer\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, output_size, activation='relu', l2_lambda=0.0001):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "        self.l2_lambda = l2_lambda\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.weights = NeuralMath.initialize_weights((input_size, output_size), activation)\n",
        "        self.biases = np.zeros((1, output_size))\n",
        "        \n",
        "        # Cache for backprop\n",
        "        self.input_cache = None\n",
        "        self.output_cache = None\n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        self.input_cache = X\n",
        "        Z = X @ self.weights + self.biases\n",
        "        \n",
        "        if self.activation == 'relu':\n",
        "            A = NeuralMath.relu(Z)\n",
        "        elif self.activation == 'softmax':\n",
        "            A = NeuralMath.softmax(Z)\n",
        "        else:\n",
        "            A = Z\n",
        "        \n",
        "        self.output_cache = A\n",
        "        return A\n",
        "    \n",
        "    def backward(self, dL_dA, learning_rate):\n",
        "        \"\"\"Backward pass\"\"\"\n",
        "        batch_size = self.input_cache.shape[0]\n",
        "        \n",
        "        if self.activation == 'relu':\n",
        "            dA_dZ = NeuralMath.relu_derivative(self.output_cache)\n",
        "            dL_dZ = dL_dA * dA_dZ\n",
        "        elif self.activation == 'softmax':\n",
        "            dL_dZ = dL_dA\n",
        "        else:\n",
        "            dL_dZ = dL_dA\n",
        "        \n",
        "        # Compute gradients\n",
        "        dL_dW = (self.input_cache.T @ dL_dZ) / batch_size\n",
        "        dL_db = np.sum(dL_dZ, axis=0, keepdims=True) / batch_size\n",
        "        \n",
        "        # Add L2 regularization\n",
        "        if self.l2_lambda > 0:\n",
        "            dL_dW += self.l2_lambda * self.weights / batch_size\n",
        "        \n",
        "        # Update parameters\n",
        "        self.weights -= learning_rate * dL_dW\n",
        "        self.biases -= learning_rate * dL_db\n",
        "        \n",
        "        # Gradient for previous layer\n",
        "        dL_dinput = dL_dZ @ self.weights.T\n",
        "        return dL_dinput\n",
        "    \n",
        "    @property\n",
        "    def num_params(self):\n",
        "        return self.weights.size + self.biases.size\n",
        "\n",
        "class DNN:\n",
        "    \"\"\"Deep Neural Network with training visualization\"\"\"\n",
        "    \n",
        "    def __init__(self, layer_sizes, activations, learning_rate=0.001, \n",
        "                 l2_lambda=0.0001, name=\"DNN\"):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activations = activations\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.name = name\n",
        "        \n",
        "        # Build layers\n",
        "        self.layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            layer = DenseLayer(\n",
        "                input_size=layer_sizes[i],\n",
        "                output_size=layer_sizes[i + 1],\n",
        "                activation=activations[i],\n",
        "                l2_lambda=l2_lambda\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "        \n",
        "        # Training history\n",
        "        self.history = {\n",
        "            'train_loss': [], 'train_acc': [],\n",
        "            'val_loss': [], 'val_acc': [],\n",
        "            'epoch_times': [],\n",
        "            'learning_rates': []\n",
        "        }\n",
        "        \n",
        "        print(f\"ðŸ§  Built {name}: {layer_sizes}\")\n",
        "        print(f\"   Parameters: {self.num_params:,}\")\n",
        "    \n",
        "    @property\n",
        "    def num_params(self):\n",
        "        return sum(layer.num_params for layer in self.layers)\n",
        "    \n",
        "    def forward(self, X, training=True):\n",
        "        \"\"\"Forward pass through all layers\"\"\"\n",
        "        activations = X\n",
        "        for layer in self.layers:\n",
        "            activations = layer.forward(activations)\n",
        "        return activations\n",
        "    \n",
        "    def compute_loss(self, y_pred, y_true):\n",
        "        \"\"\"Compute total loss\"\"\"\n",
        "        loss = NeuralMath.cross_entropy(y_pred, y_true)\n",
        "        \n",
        "        # Add L2 regularization\n",
        "        if self.l2_lambda > 0:\n",
        "            reg_loss = 0\n",
        "            for layer in self.layers:\n",
        "                reg_loss += np.sum(layer.weights ** 2)\n",
        "            loss += (self.l2_lambda / (2 * y_true.shape[0])) * reg_loss\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def compute_accuracy(self, y_pred, y_true):\n",
        "        \"\"\"Compute accuracy\"\"\"\n",
        "        pred_labels = np.argmax(y_pred, axis=1)\n",
        "        true_labels = np.argmax(y_true, axis=1)\n",
        "        return np.mean(pred_labels == true_labels)\n",
        "    \n",
        "    def train_epoch(self, X_batch, y_batch):\n",
        "        \"\"\"Single training step\"\"\"\n",
        "        # Forward pass\n",
        "        y_pred = self.forward(X_batch)\n",
        "        \n",
        "        # Compute gradient\n",
        "        loss_grad = NeuralMath.cross_entropy_gradient(y_pred, y_batch)\n",
        "        \n",
        "        # Backward pass\n",
        "        grad = loss_grad\n",
        "        for layer in reversed(self.layers):\n",
        "            grad = layer.backward(grad, self.learning_rate)\n",
        "        \n",
        "        # Compute metrics\n",
        "        loss = self.compute_loss(y_pred, y_batch)\n",
        "        accuracy = self.compute_accuracy(y_pred, y_batch)\n",
        "        \n",
        "        return loss, accuracy\n",
        "    \n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
        "              epochs=20, batch_size=64, verbose=True):\n",
        "        \"\"\"Training loop with progress bars\"\"\"\n",
        "        n_samples = X_train.shape[0]\n",
        "        n_batches = int(np.ceil(n_samples / batch_size))\n",
        "        \n",
        "        print(f\"ðŸš€ Training {self.name} for {epochs} epochs\")\n",
        "        print(f\"   Samples: {n_samples:,}, Batch size: {batch_size}, Batches/epoch: {n_batches}\")\n",
        "        print(\"-\" * 70)\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            epoch_start = time.time()\n",
        "            epoch_loss, epoch_acc = 0, 0\n",
        "            \n",
        "            # Shuffle data\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X_train[indices]\n",
        "            y_shuffled = y_train[indices]\n",
        "            \n",
        "            # Mini-batch training with progress bar\n",
        "            with tqdm(total=n_batches, desc=f\"Epoch {epoch+1}/{epochs}\", \n",
        "                     bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}',\n",
        "                     leave=False) as pbar:\n",
        "                for batch in range(n_batches):\n",
        "                    start = batch * batch_size\n",
        "                    end = min(start + batch_size, n_samples)\n",
        "                    \n",
        "                    X_batch = X_shuffled[start:end]\n",
        "                    y_batch = y_shuffled[start:end]\n",
        "                    \n",
        "                    batch_loss, batch_acc = self.train_epoch(X_batch, y_batch)\n",
        "                    epoch_loss += batch_loss\n",
        "                    epoch_acc += batch_acc\n",
        "                    \n",
        "                    # Update progress bar\n",
        "                    pbar.set_postfix({\n",
        "                        'loss': f'{batch_loss:.4f}',\n",
        "                        'acc': f'{batch_acc:.2%}'\n",
        "                    })\n",
        "                    pbar.update(1)\n",
        "            \n",
        "            # Average over batches\n",
        "            epoch_loss /= n_batches\n",
        "            epoch_acc /= n_batches\n",
        "            \n",
        "            # Validation\n",
        "            if X_val is not None and y_val is not None:\n",
        "                y_val_pred = self.forward(X_val)\n",
        "                val_loss = self.compute_loss(y_val_pred, y_val)\n",
        "                val_acc = self.compute_accuracy(y_val_pred, y_val)\n",
        "            else:\n",
        "                val_loss, val_acc = None, None\n",
        "            \n",
        "            # Record history\n",
        "            self.history['train_loss'].append(epoch_loss)\n",
        "            self.history['train_acc'].append(epoch_acc)\n",
        "            self.history['epoch_times'].append(time.time() - epoch_start)\n",
        "            self.history['learning_rates'].append(self.learning_rate)\n",
        "            \n",
        "            if val_loss is not None:\n",
        "                self.history['val_loss'].append(val_loss)\n",
        "                self.history['val_acc'].append(val_acc)\n",
        "            \n",
        "            # Progress report\n",
        "            if verbose:\n",
        "                if val_loss is not None:\n",
        "                    print(f\"Epoch {epoch+1:3d} | Train: loss={epoch_loss:.4f}, acc={epoch_acc:.2%} | \"\n",
        "                          f\"Val: loss={val_loss:.4f}, acc={val_acc:.2%} | Time: {self.history['epoch_times'][-1]:.1f}s\")\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1:3d} | Train: loss={epoch_loss:.4f}, acc={epoch_acc:.2%} | \"\n",
        "                          f\"Time: {self.history['epoch_times'][-1]:.1f}s\")\n",
        "        \n",
        "        print(\"-\" * 70)\n",
        "        avg_time = np.mean(self.history['epoch_times'])\n",
        "        print(f\"âœ… Training complete. Avg time/epoch: {avg_time:.2f}s\")\n",
        "    \n",
        "    def evaluate(self, X, y):\n",
        "        \"\"\"Evaluate model\"\"\"\n",
        "        y_pred = self.forward(X)\n",
        "        loss = self.compute_loss(y_pred, y)\n",
        "        accuracy = self.compute_accuracy(y_pred, y)\n",
        "        return loss, accuracy\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        y_pred = self.forward(X)\n",
        "        return np.argmax(y_pred, axis=1), y_pred\n",
        "    \n",
        "    def save(self, filename):\n",
        "        \"\"\"Save model to disk\"\"\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "        print(f\"ðŸ’¾ Saved: {filename}\")\n",
        "    \n",
        "    @staticmethod\n",
        "    def load(filename):\n",
        "        \"\"\"Load model from disk\"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        print(f\"ðŸ“‚ Loaded: {filename}\")\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š PART 4: Training Visualization System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "class TrainingVisualizer:\n",
        "    \"\"\"Comprehensive training visualization\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def plot_training_history(history, model_name, dataset_name, save_path=None):\n",
        "        \"\"\"Create 2x2 visualization grid\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "        \n",
        "        epochs = range(1, len(history['train_loss']) + 1)\n",
        "        \n",
        "        # 1. Loss curves\n",
        "        ax = axes[0, 0]\n",
        "        ax.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "        if history['val_loss']:\n",
        "            ax.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.set_title('Loss Curves')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 2. Accuracy curves\n",
        "        ax = axes[0, 1]\n",
        "        ax.plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
        "        if history['val_acc']:\n",
        "            ax.plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Accuracy')\n",
        "        ax.set_title('Accuracy Curves')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 3. Training time per epoch\n",
        "        ax = axes[1, 0]\n",
        "        ax.plot(epochs, history['epoch_times'], 'purple', marker='o', alpha=0.7)\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Time (seconds)')\n",
        "        ax.set_title('Training Time per Epoch')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 4. Loss-Accuracy tradeoff\n",
        "        ax = axes[1, 1]\n",
        "        scatter = ax.scatter(history['train_loss'], history['train_acc'], \n",
        "                           c=epochs, cmap='viridis', s=50, alpha=0.7)\n",
        "        ax.set_xlabel('Loss')\n",
        "        ax.set_ylabel('Accuracy')\n",
        "        ax.set_title('Loss vs Accuracy (Epochs)')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.colorbar(scatter, ax=ax, label='Epoch')\n",
        "        \n",
        "        plt.suptitle(f'{model_name} on {dataset_name}', \n",
        "                    fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    @staticmethod\n",
        "    def plot_all_models_comparison(all_results):\n",
        "        \"\"\"Compare all 9 models\"\"\"\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        \n",
        "        # Extract data\n",
        "        model_names = []\n",
        "        datasets = []\n",
        "        test_accuracies = []\n",
        "        parameters = []\n",
        "        \n",
        "        for (model_name, dataset_name), results in all_results.items():\n",
        "            model_names.append(model_name)\n",
        "            datasets.append(dataset_name)\n",
        "            test_accuracies.append(results['test_accuracy'])\n",
        "            parameters.append(results['parameters'])\n",
        "        \n",
        "        # Color by dataset\n",
        "        dataset_colors = {'mnist': 'blue', 'fashion': 'orange', 'cifar10': 'green'}\n",
        "        colors = [dataset_colors[d] for d in datasets]\n",
        "        \n",
        "        # 1. Test accuracy comparison\n",
        "        x_pos = np.arange(len(model_names))\n",
        "        bars = axes[0].bar(x_pos, test_accuracies, color=colors, edgecolor='black')\n",
        "        axes[0].set_xlabel('Model')\n",
        "        axes[0].set_ylabel('Test Accuracy')\n",
        "        axes[0].set_title('Test Accuracy Comparison (9 Models)')\n",
        "        axes[0].set_xticks(x_pos)\n",
        "        axes[0].set_xticklabels([f\"{m}\\n({d})\" for m, d in zip(model_names, datasets)], \n",
        "                               rotation=45, ha='right', fontsize=8)\n",
        "        axes[0].set_ylim([0, 1])\n",
        "        axes[0].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        # Add accuracy labels\n",
        "        for bar, acc in zip(bars, test_accuracies):\n",
        "            height = bar.get_height()\n",
        "            axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                       f'{acc:.1%}', ha='center', va='bottom', fontsize=8)\n",
        "        \n",
        "        # 2. Parameters vs Accuracy\n",
        "        scatter = axes[1].scatter(parameters, test_accuracies, s=100, c=colors, \n",
        "                                 edgecolor='black', alpha=0.7)\n",
        "        axes[1].set_xlabel('Number of Parameters')\n",
        "        axes[1].set_ylabel('Test Accuracy')\n",
        "        axes[1].set_title('Parameters vs Accuracy Efficiency')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        axes[1].set_xscale('log')\n",
        "        \n",
        "        # Add labels for each point\n",
        "        for i, (model, dataset, param, acc) in enumerate(zip(model_names, datasets, parameters, test_accuracies)):\n",
        "            axes[1].annotate(f\"{model}\\n{dataset}\", \n",
        "                           (param, acc), \n",
        "                           xytext=(5, 5), \n",
        "                           textcoords='offset points',\n",
        "                           fontsize=7,\n",
        "                           bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.7))\n",
        "        \n",
        "        # 3. Dataset performance comparison\n",
        "        ax = axes[2]\n",
        "        dataset_means = {}\n",
        "        for dataset in ['mnist', 'fashion', 'cifar10']:\n",
        "            dataset_accs = [acc for d, acc in zip(datasets, test_accuracies) if d == dataset]\n",
        "            if dataset_accs:\n",
        "                dataset_means[dataset] = np.mean(dataset_accs)\n",
        "        \n",
        "        bars_ds = ax.bar(range(len(dataset_means)), list(dataset_means.values()), \n",
        "                        color=[dataset_colors[d] for d in dataset_means.keys()], \n",
        "                        edgecolor='black')\n",
        "        ax.set_xlabel('Dataset')\n",
        "        ax.set_ylabel('Average Test Accuracy')\n",
        "        ax.set_title('Dataset Difficulty Comparison')\n",
        "        ax.set_xticks(range(len(dataset_means)))\n",
        "        ax.set_xticklabels([f\"{d.upper()}\\n({len([x for x in datasets if x == d])} models)\" \n",
        "                           for d in dataset_means.keys()])\n",
        "        ax.set_ylim([0, 1])\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        for bar, acc in zip(bars_ds, dataset_means.values()):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                   f'{acc:.1%}', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        plt.suptitle('3Ã—3 Experiment Matrix: 3 Architectures Ã— 3 Datasets', \n",
        "                    fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(VIZ_DIR / 'all_models_comparison.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Print summary table\n",
        "        print(\"\\nðŸ“Š SUMMARY TABLE: 3Ã—3 EXPERIMENT MATRIX\")\n",
        "        print(\"=\" * 85)\n",
        "        print(f\"{'Model':<20} {'Dataset':<15} {'Test Acc':<12} {'Params':<15} {'Params/Acc':<15}\")\n",
        "        print(\"-\" * 85)\n",
        "        \n",
        "        for (model_name, dataset_name), results in all_results.items():\n",
        "            params = results['parameters']\n",
        "            acc = results['test_accuracy']\n",
        "            efficiency = params / (acc + 1e-8)\n",
        "            \n",
        "            print(f\"{model_name:<20} {dataset_name:<15} {acc:<12.2%} {params:<15,} {efficiency:,.0f}\")\n",
        "        \n",
        "        print(\"-\" * 85)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§ª PART 5: 3Ã—3 Experiment Matrix (9 Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Define 3 architectures\n",
        "ARCHITECTURES = {\n",
        "    'simple_dnn': {\n",
        "        'layer_sizes': [None, 64, 10],  # None will be replaced with input size\n",
        "        'activations': ['relu', 'softmax'],\n",
        "        'learning_rate': 0.001,\n",
        "        'l2_lambda': 0.0001,\n",
        "        'epochs': 20\n",
        "    },\n",
        "    'medium_dnn': {\n",
        "        'layer_sizes': [None, 128, 64, 10],\n",
        "        'activations': ['relu', 'relu', 'softmax'],\n",
        "        'learning_rate': 0.0005,\n",
        "        'l2_lambda': 0.0005,\n",
        "        'epochs': 25\n",
        "    },\n",
        "    'deep_dnn': {\n",
        "        'layer_sizes': [None, 256, 128, 64, 10],\n",
        "        'activations': ['relu', 'relu', 'relu', 'softmax'],\n",
        "        'learning_rate': 0.0003,\n",
        "        'l2_lambda': 0.001,\n",
        "        'epochs': 30\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nðŸ”¬ RUNNING 3Ã—3 EXPERIMENT MATRIX (9 MODELS)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "all_models = {}\n",
        "all_results = {}\n",
        "\n",
        "for arch_name, arch_config in ARCHITECTURES.items():\n",
        "    print(f\"\\nðŸ—ï¸  ARCHITECTURE: {arch_name.upper()}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for dataset_name in ['mnist', 'fashion', 'cifar10']:\n",
        "        # Get data\n",
        "        data_dict = prepared_data[dataset_name]['data']\n",
        "        input_size = data_dict['X_train'].shape[1]\n",
        "        \n",
        "        # Update config with input size\n",
        "        config = arch_config.copy()\n",
        "        config['layer_sizes'][0] = input_size\n",
        "        \n",
        "        # Create model\n",
        "        model_name = f\"{arch_name}_{dataset_name}\"\n",
        "        model = DNN(\n",
        "            layer_sizes=config['layer_sizes'],\n",
        "            activations=config['activations'],\n",
        "            learning_rate=config['learning_rate'],\n",
        "            l2_lambda=config['l2_lambda'],\n",
        "            name=model_name\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nðŸ“Š Training {model_name}:\")\n",
        "        \n",
        "        # Train model\n",
        "        model.train(\n",
        "            X_train=data_dict['X_train'],\n",
        "            y_train=data_dict['y_train'],\n",
        "            X_val=data_dict['X_val'],\n",
        "            y_val=data_dict['y_val'],\n",
        "            epochs=config['epochs'],\n",
        "            batch_size=64,\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        # Evaluate\n",
        "        test_loss, test_acc = model.evaluate(data_dict['X_test'], data_dict['y_test'])\n",
        "        \n",
        "        # Store results\n",
        "        all_models[model_name] = model\n",
        "        all_results[(arch_name, dataset_name)] = {\n",
        "            'model': model_name,\n",
        "            'test_accuracy': test_acc,\n",
        "            'test_loss': test_loss,\n",
        "            'parameters': model.num_params,\n",
        "            'history': model.history\n",
        "        }\n",
        "        \n",
        "        print(f\"   Test Accuracy: {test_acc:.2%}, Test Loss: {test_loss:.4f}\")\n",
        "        \n",
        "        # Save model\n",
        "        model.save(MODELS_DIR / f\"{model_name}.pkl\")\n",
        "        \n",
        "        # Visualize training\n",
        "        TrainingVisualizer.plot_training_history(\n",
        "            model.history,\n",
        "            model_name=model_name,\n",
        "            dataset_name=dataset_name.upper(),\n",
        "            save_path=VIZ_DIR / f\"training_{model_name}.png\"\n",
        "        )\n",
        "\n",
        "print(\"\\nâœ… ALL 9 MODELS TRAINED AND SAVED!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ˆ PART 6: Comprehensive Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Compare all models\n",
        "TrainingVisualizer.plot_all_models_comparison(all_results)\n",
        "\n",
        "# Key insights\n",
        "print(\"\\nðŸ” KEY INSIGHTS FROM 3Ã—3 EXPERIMENT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate dataset averages\n",
        "dataset_stats = {}\n",
        "for dataset in ['mnist', 'fashion', 'cifar10']:\n",
        "    dataset_accs = [results['test_accuracy'] for (arch, ds), results in all_results.items() if ds == dataset]\n",
        "    dataset_stats[dataset] = {\n",
        "        'mean_accuracy': np.mean(dataset_accs),\n",
        "        'std_accuracy': np.std(dataset_accs),\n",
        "        'best_accuracy': np.max(dataset_accs),\n",
        "        'worst_accuracy': np.min(dataset_accs)\n",
        "    }\n",
        "\n",
        "# Print insights\n",
        "insights = [\n",
        "    (\"ðŸ“‰ Performance Drop with Complexity\",\n",
        "     f\"MNIST: {dataset_stats['mnist']['mean_accuracy']:.1%} avg â†’ \"\n",
        "     f\"Fashion MNIST: {dataset_stats['fashion']['mean_accuracy']:.1%} avg â†’ \"\n",
        "     f\"CIFAR-10: {dataset_stats['cifar10']['mean_accuracy']:.1%} avg\\n\"\n",
        "     \"   DNNs struggle as data complexity increases.\"),\n",
        "    \n",
        "    (\"ðŸ—ï¸ Diminishing Returns with Depth\",\n",
        "     \"Deep DNN (3 hidden) is only ~5-10% better than Simple DNN (1 hidden)\\n\"\n",
        "     \"   but has 3-4x more parameters â†’ poor parameter efficiency.\"),\n",
        "    \n",
        "    (\"ðŸŽ¨ RGB â†’ Grayscale Information Loss\",\n",
        "     \"CIFAR-10 converted to grayscale loses critical color information.\\n\"\n",
        "     \"   A red car vs blue car looks identical to DNN.\"),\n",
        "    \n",
        "    (\"ðŸ”„ Translation Sensitivity\",\n",
        "     \"A shirt in top-left vs bottom-right appears completely different.\\n\"\n",
        "     \"   DNNs have no built-in translation invariance.\"),\n",
        "    \n",
        "    (\"ðŸ“Š Parameter Explosion\",\n",
        "     f\"CIFAR-10 input: 32Ã—32Ã—3 = 3,072 pixels â†’ ~{all_models['deep_dnn_cifar10'].num_params:,} params\\n\"\n",
        "     \"   MNIST input: 28Ã—28 = 784 pixels â†’ much fewer params\\n\"\n",
        "     \"   Yet accuracy is lower with more parameters!\"),\n",
        "    \n",
        "    (\"ðŸ’¡ Why CNNs Are Needed\",\n",
        "     \"â€¢ Parameter sharing â†’ efficiency\\n\"\n",
        "     \"â€¢ Translation invariance â†’ robustness\\n\"\n",
        "     \"â€¢ Hierarchical features â†’ better learning\\n\"\n",
        "     \"â€¢ Spatial preservation â†’ understand relationships\")\n",
        "]\n",
        "\n",
        "for i, (title, content) in enumerate(insights, 1):\n",
        "    print(f\"\\n{i}. {title}\")\n",
        "    print(content.replace('\\n', '\\n   '))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸŽ¯ CONCLUSION: DNNs work for simple patterns but fail dramatically\\n\"\n",
        "      \"          for complex spatial data. This clearly demonstrates why\\n\"\n",
        "      \"          Convolutional Neural Networks (CNNs) were invented.\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ—ï¸ PART 7: Pipeline Visualization System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from matplotlib.patches import FancyBboxPatch, Circle\n",
        "\n",
        "class PipelineVisualizer:\n",
        "    \"\"\"Visualize deep learning pipeline architecture\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def visualize_complete_pipeline():\n",
        "        \"\"\"Create comprehensive pipeline visualization\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        \n",
        "        # 1. Data Flow Diagram\n",
        "        ax1 = axes[0, 0]\n",
        "        PipelineVisualizer._visualize_data_flow(ax1)\n",
        "        \n",
        "        # 2. Training Pipeline\n",
        "        ax2 = axes[0, 1]\n",
        "        PipelineVisualizer._visualize_training_pipeline(ax2)\n",
        "        \n",
        "        # 3. Model Architecture\n",
        "        ax3 = axes[0, 2]\n",
        "        PipelineVisualizer._visualize_model_architecture(ax3)\n",
        "        \n",
        "        # 4. System Components\n",
        "        ax4 = axes[1, 0]\n",
        "        PipelineVisualizer._visualize_system_components(ax4)\n",
        "        \n",
        "        # 5. Production Deployment\n",
        "        ax5 = axes[1, 1]\n",
        "        PipelineVisualizer._visualize_deployment_pipeline(ax5)\n",
        "        \n",
        "        # 6. Monitoring & Maintenance\n",
        "        ax6 = axes[1, 2]\n",
        "        PipelineVisualizer._visualize_monitoring_pipeline(ax6)\n",
        "        \n",
        "        plt.suptitle(\"Deep Learning Pipeline: Complete System Architecture\", \n",
        "                    fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(VIZ_DIR / \"pipeline_architecture.png\", dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    @staticmethod\n",
        "    def _visualize_data_flow(ax):\n",
        "        \"\"\"Visualize data flow through pipeline\"\"\"\n",
        "        stages = [\n",
        "            \"Raw Data\",\n",
        "            \"Data Loader\",\n",
        "            \"Preprocessor\",\n",
        "            \"Augmentations\",\n",
        "            \"Data Splits\",\n",
        "            \"Data Loaders\"\n",
        "        ]\n",
        "        \n",
        "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']\n",
        "        \n",
        "        for i, (stage, color) in enumerate(zip(stages, colors)):\n",
        "            # Create rounded rectangle\n",
        "            bbox = FancyBboxPatch((0.1, 0.8 - i*0.15), 0.8, 0.1,\n",
        "                                 boxstyle=\"round,pad=0.02\",\n",
        "                                 facecolor=color, edgecolor='black',\n",
        "                                 linewidth=2)\n",
        "            ax.add_patch(bbox)\n",
        "            ax.text(0.5, 0.85 - i*0.15, stage,\n",
        "                   ha='center', va='center', fontsize=10, fontweight='bold')\n",
        "            \n",
        "            # Add arrows\n",
        "            if i < len(stages) - 1:\n",
        "                ax.arrow(0.5, 0.75 - i*0.15, 0, -0.1,\n",
        "                        head_width=0.03, head_length=0.02,\n",
        "                        fc='black', ec='black')\n",
        "        \n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_title(\"ðŸ“Š Data Flow Pipeline\", fontsize=12, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "    \n",
        "    @staticmethod\n",
        "    def _visualize_training_pipeline(ax):\n",
        "        \"\"\"Visualize training pipeline\"\"\"\n",
        "        # Create directed graph\n",
        "        G = nx.DiGraph()\n",
        "        \n",
        "        nodes = [\n",
        "            \"Data Loader\", \"Model\", \"Loss\",\n",
        "            \"Optimizer\", \"Backward Pass\", \"Parameter Update\",\n",
        "            \"Validation\", \"Checkpoint\", \"Logging\"\n",
        "        ]\n",
        "        \n",
        "        edges = [\n",
        "            (\"Data Loader\", \"Model\"),\n",
        "            (\"Model\", \"Loss\"),\n",
        "            (\"Loss\", \"Backward Pass\"),\n",
        "            (\"Backward Pass\", \"Optimizer\"),\n",
        "            (\"Optimizer\", \"Parameter Update\"),\n",
        "            (\"Parameter Update\", \"Model\"),  # Next iteration\n",
        "            (\"Model\", \"Validation\"),\n",
        "            (\"Validation\", \"Checkpoint\"),\n",
        "            (\"Validation\", \"Logging\")\n",
        "        ]\n",
        "        \n",
        "        G.add_nodes_from(nodes)\n",
        "        G.add_edges_from(edges)\n",
        "        \n",
        "        # Position nodes\n",
        "        pos = {\n",
        "            \"Data Loader\": (0.2, 0.8),\n",
        "            \"Model\": (0.5, 0.8),\n",
        "            \"Loss\": (0.8, 0.8),\n",
        "            \"Backward Pass\": (0.8, 0.6),\n",
        "            \"Optimizer\": (0.5, 0.6),\n",
        "            \"Parameter Update\": (0.2, 0.6),\n",
        "            \"Validation\": (0.5, 0.4),\n",
        "            \"Checkpoint\": (0.2, 0.4),\n",
        "            \"Logging\": (0.8, 0.4)\n",
        "        }\n",
        "        \n",
        "        # Draw graph\n",
        "        nx.draw_networkx_nodes(G, pos, node_color='lightblue', \n",
        "                              node_size=2000, ax=ax, alpha=0.8)\n",
        "        nx.draw_networkx_edges(G, pos, edge_color='gray', \n",
        "                              arrows=True, arrowsize=20, ax=ax, width=2)\n",
        "        nx.draw_networkx_labels(G, pos, font_size=8, ax=ax, font_weight='bold')\n",
        "        \n",
        "        ax.set_title(\"ðŸš€ Training Pipeline\", fontsize=12, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "    \n",
        "    @staticmethod\n",
        "    def _visualize_model_architecture(ax):\n",
        "        \"\"\"Visualize model architecture\"\"\"\n",
        "        # Create a simple neural network visualization\n",
        "        layers = [4, 8, 6, 2]  # Layer sizes\n",
        "        \n",
        "        # Calculate positions\n",
        "        x_positions = np.linspace(0.1, 0.9, len(layers))\n",
        "        \n",
        "        for layer_idx, (x, n_neurons) in enumerate(zip(x_positions, layers)):\n",
        "            y_positions = np.linspace(0.1, 0.9, n_neurons)\n",
        "            \n",
        "            for y in y_positions:\n",
        "                circle = Circle((x, y), 0.03, color='skyblue', ec='black', lw=1)\n",
        "                ax.add_patch(circle)\n",
        "            \n",
        "            # Draw layer label\n",
        "            layer_type = [\"Input\", \"Hidden\", \"Hidden\", \"Output\"][layer_idx]\n",
        "            ax.text(x, 0.05, f\"{layer_type}\\n({n_neurons})\", \n",
        "                   ha='center', va='center', fontsize=9)\n",
        "        \n",
        "        # Draw connections between layers\n",
        "        for i in range(len(layers) - 1):\n",
        "            x1 = x_positions[i]\n",
        "            x2 = x_positions[i + 1]\n",
        "            y1_positions = np.linspace(0.1, 0.9, layers[i])\n",
        "            y2_positions = np.linspace(0.1, 0.9, layers[i + 1])\n",
        "            \n",
        "            # Draw a few sample connections\n",
        "            for y1 in y1_positions[:3]:  # Only show first 3 neurons' connections\n",
        "                for y2 in y2_positions[:3]:\n",
        "                    ax.plot([x1, x2], [y1, y2], 'gray', alpha=0.3, linewidth=0.5)\n",
        "        \n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_title(\"ðŸ§  Model Architecture\", fontsize=12, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "    \n",
        "    @staticmethod\n",
        "    def _visualize_system_components(ax):\n",
        "        \"\"\"Visualize system component relationships\"\"\"\n",
        "        components = {\n",
        "            \"Core\": [\"Data Module\", \"Model Module\", \"Training Module\"],\n",
        "            \"Support\": [\"Config Manager\", \"Logger\", \"Monitor\"],\n",
        "            \"Infrastructure\": [\"Experiment Tracker\", \"Model Registry\", \"Serving Engine\"]\n",
        "        }\n",
        "        \n",
        "        y_pos = 0.9\n",
        "        for category, items in components.items():\n",
        "            ax.text(0.1, y_pos, f\"{category}:\", \n",
        "                   fontsize=10, fontweight='bold', va='center')\n",
        "            \n",
        "            for i, item in enumerate(items):\n",
        "                ax.text(0.3, y_pos - (i+1)*0.08, f\"â€¢ {item}\", \n",
        "                       fontsize=9, va='center')\n",
        "            \n",
        "            y_pos -= (len(items) + 1) * 0.08\n",
        "        \n",
        "        # Add connections\n",
        "        ax.plot([0.6, 0.8], [0.7, 0.7], 'k-', alpha=0.5)\n",
        "        ax.plot([0.6, 0.8], [0.5, 0.5], 'k-', alpha=0.5)\n",
        "        ax.plot([0.6, 0.8], [0.3, 0.3], 'k-', alpha=0.5)\n",
        "        \n",
        "        ax.text(0.85, 0.7, \"API Layer\", fontsize=9, ha='center', \n",
        "               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
        "        \n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_title(\"ðŸ—ï¸ System Components\", fontsize=12, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "    \n",
        "    @staticmethod\n",
        "    def _visualize_deployment_pipeline(ax):\n",
        "        \"\"\"Visualize production deployment pipeline\"\"\"\n",
        "        stages = [\n",
        "            (\"Model Training\", 0.2, 0.9),\n",
        "            (\"Model Validation\", 0.5, 0.9),\n",
        "            (\"Model Export\", 0.8, 0.9),\n",
        "            (\"Containerization\", 0.2, 0.7),\n",
        "            (\"API Development\", 0.5, 0.7),\n",
        "            (\"Load Testing\", 0.8, 0.7),\n",
        "            (\"Cloud Deployment\", 0.2, 0.5),\n",
        "            (\"Auto-scaling\", 0.5, 0.5),\n",
        "            (\"Monitoring Setup\", 0.8, 0.5),\n",
        "            (\"Production\", 0.5, 0.3)\n",
        "        ]\n",
        "        \n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(stages)))\n",
        "        \n",
        "        for (stage, x, y), color in zip(stages, colors):\n",
        "            # Draw circle for each stage\n",
        "            circle = Circle((x, y), 0.06, color=color, ec='black', lw=1.5)\n",
        "            ax.add_patch(circle)\n",
        "            ax.text(x, y, stage.replace(' ', '\\n'), \n",
        "                   ha='center', va='center', fontsize=7, fontweight='bold')\n",
        "        \n",
        "        # Draw flow arrows\n",
        "        arrows = [\n",
        "            ((0.2, 0.83), (0.2, 0.77)),  # Training -> Containerization\n",
        "            ((0.5, 0.83), (0.5, 0.77)),  # Validation -> API Dev\n",
        "            ((0.8, 0.83), (0.8, 0.77)),  # Export -> Load Testing\n",
        "            ((0.2, 0.63), (0.2, 0.57)),  # Containerization -> Cloud\n",
        "            ((0.5, 0.63), (0.5, 0.57)),  # API Dev -> Auto-scaling\n",
        "            ((0.8, 0.63), (0.8, 0.57)),  # Load Testing -> Monitoring\n",
        "            ((0.2, 0.43), (0.4, 0.37)),  # Cloud -> Production\n",
        "            ((0.5, 0.43), (0.5, 0.37)),  # Auto-scaling -> Production\n",
        "            ((0.8, 0.43), (0.6, 0.37))   # Monitoring -> Production\n",
        "        ]\n",
        "        \n",
        "        for (x1, y1), (x2, y2) in arrows:\n",
        "            ax.arrow(x1, y1, x2-x1, y2-y1, \n",
        "                    head_width=0.02, head_length=0.03,\n",
        "                    fc='black', ec='black', alpha=0.7)\n",
        "        \n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_title(\"ðŸš€ Production Deployment\", fontsize=12, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "    \n",
        "    @staticmethod\n",
        "    def _visualize_monitoring_pipeline(ax):\n",
        "        \"\"\"Visualize monitoring and maintenance pipeline\"\"\"\n",
        "        # Create a monitoring dashboard visualization\n",
        "        metrics = [\"Accuracy\", \"Loss\", \"Latency\", \"Memory\", \"CPU\", \"GPU\"]\n",
        "        values = [0.85, 0.32, 45, 78, 65, 92]  # Example values\n",
        "        \n",
        "        # Create gauge-like indicators\n",
        "        angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)\n",
        "        radius = 0.35\n",
        "        \n",
        "        for metric, value, angle in zip(metrics, values, angles):\n",
        "            # Normalize value for display (0-100%)\n",
        "            norm_value = value / 100 if metric in [\"Memory\", \"CPU\", \"GPU\"] else value\n",
        "            \n",
        "            # Calculate position\n",
        "            x = 0.5 + radius * np.cos(angle)\n",
        "            y = 0.5 + radius * np.sin(angle)\n",
        "            \n",
        "            # Draw gauge background\n",
        "            gauge_bg = Circle((x, y), 0.08, color='lightgray', ec='black', alpha=0.3)\n",
        "            ax.add_patch(gauge_bg)\n",
        "            \n",
        "            # Draw gauge fill\n",
        "            fill_radius = 0.08 * norm_value\n",
        "            gauge_fill = Circle((x, y), fill_radius, color='green' if norm_value > 0.7 else \n",
        "                               'orange' if norm_value > 0.4 else 'red', \n",
        "                               alpha=0.7)\n",
        "            ax.add_patch(gauge_fill)\n",
        "            \n",
        "            # Add metric label\n",
        "            ax.text(x, y - 0.12, metric, ha='center', va='center', fontsize=8)\n",
        "            ax.text(x, y, f\"{value:.1f}{'%' if metric in ['Memory', 'CPU', 'GPU'] else ''}\", \n",
        "                   ha='center', va='center', fontsize=9, fontweight='bold')\n",
        "        \n",
        "        # Add central title\n",
        "        ax.text(0.5, 0.5, \"Live\\nMonitoring\", ha='center', va='center',\n",
        "               fontsize=10, fontweight='bold', \n",
        "               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "        \n",
        "        # Add alerts section\n",
        "        ax.text(0.15, 0.15, \"ðŸ”´ High Loss\\nðŸŸ¡ Memory 78%\\nðŸŸ¢ System OK\", \n",
        "               fontsize=8, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\"))\n",
        "        \n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_title(\"ðŸ“ˆ Monitoring & Maintenance\", fontsize=12, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "\n",
        "# Show pipeline visualization\n",
        "print(\"\\nðŸ—ï¸ VISUALIZING PRODUCTION PIPELINE ARCHITECTURE\")\n",
        "PipelineVisualizer.visualize_complete_pipeline()\n",
        "\n",
        "# Pipeline design principles\n",
        "print(\"\\nðŸ“‹ PIPELINE DESIGN PRINCIPLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "principles = [\n",
        "    \"1. Modularity: Each component does one thing well\",\n",
        "    \"2. Reproducibility: Same code â†’ same results\",\n",
        "    \"3. Testability: Unit test each component\",\n",
        "    \"4. Monitoring: Track everything, alert on anomalies\",\n",
        "    \"5. Versioning: Data, code, models, and configs\",\n",
        "    \"6. Automation: Manual steps are failure points\",\n",
        "    \"7. Scalability: Handle 10x data/models without rewrite\",\n",
        "    \"8. Observability: Understand why models fail\"\n",
        "]\n",
        "\n",
        "for principle in principles:\n",
        "    print(f\"   {principle}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ PART 8: Interactive Model Inference UI (Gradio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "import sys\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "class ModelInferenceUI:\n",
        "    \"\"\"Interactive UI for comparing our 9 models\"\"\"\n",
        "    \n",
        "    def __init__(self, all_models, prepared_data):\n",
        "        self.models = all_models\n",
        "        self.prepared_data = prepared_data\n",
        "        self.results = all_results\n",
        "        \n",
        "        # Get best model for each dataset\n",
        "        self.best_models = self._get_best_models()\n",
        "        \n",
        "        print(\"\\nðŸ“± PREPARING INTERACTIVE INFERENCE UI\")\n",
        "        print(f\"   Total models: {len(self.models)}\")\n",
        "        print(f\"   Best models per dataset: {list(self.best_models.keys())}\")\n",
        "    \n",
        "    def _get_best_models(self):\n",
        "        \"\"\"Find best model for each dataset\"\"\"\n",
        "        best_models = {}\n",
        "        \n",
        "        for dataset in ['mnist', 'fashion', 'cifar10']:\n",
        "            # Find models for this dataset\n",
        "            dataset_models = {k: v for k, v in self.results.items() if k[1] == dataset}\n",
        "            \n",
        "            if dataset_models:\n",
        "                # Get model with highest accuracy\n",
        "                best_key = max(dataset_models, key=lambda k: dataset_models[k]['test_accuracy'])\n",
        "                best_model_name = f\"{best_key[0]}_{best_key[1]}\"\n",
        "                best_models[dataset] = {\n",
        "                    'name': best_model_name,\n",
        "                    'model': self.models[best_model_name],\n",
        "                    'accuracy': dataset_models[best_key]['test_accuracy']\n",
        "                }\n",
        "        \n",
        "        return best_models\n",
        "    \n",
        "    def create_ui(self):\n",
        "        \"\"\"Create Gradio UI\"\"\"\n",
        "        try:\n",
        "            import gradio as gr\n",
        "        except ImportError:\n",
        "            print(\"Installing Gradio...\")\n",
        "            import subprocess\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gradio\"])\n",
        "            import gradio as gr\n",
        "        \n",
        "        # Prepare sample images\n",
        "        sample_images = self._prepare_sample_images()\n",
        "        \n",
        "        def predict(image, selected_dataset):\n",
        "            \"\"\"Predict using best model for selected dataset\"\"\"\n",
        "            if image is None:\n",
        "                return \"Please upload or select an image\", {}, None\n",
        "            \n",
        "            try:\n",
        "                # Get best model for dataset\n",
        "                if selected_dataset not in self.best_models:\n",
        "                    return f\"No model available for {selected_dataset}\", {}, None\n",
        "                \n",
        "                model_info = self.best_models[selected_dataset]\n",
        "                model = model_info['model']\n",
        "                \n",
        "                # Preprocess image\n",
        "                processed = self._preprocess_image(image, selected_dataset)\n",
        "                \n",
        "                # Predict\n",
        "                pred_labels, pred_probs = model.predict(processed)\n",
        "                \n",
        "                # Get class names\n",
        "                classes = self.prepared_data[selected_dataset]['classes']\n",
        "                pred_class = classes[pred_labels[0]]\n",
        "                confidence = pred_probs[0][pred_labels[0]]\n",
        "                \n",
        "                # Create probabilities dict\n",
        "                probabilities = {classes[i]: float(pred_probs[0][i]) for i in range(len(classes))}\n",
        "                sorted_probs = dict(sorted(probabilities.items(), key=lambda x: x[1], reverse=True))\n",
        "                \n",
        "                # Create visualization\n",
        "                fig = self._create_probability_plot(probabilities, pred_class, selected_dataset)\n",
        "                \n",
        "                result_text = (\n",
        "                    f\"**Prediction:** {pred_class} ({confidence:.1%})\\n\"\n",
        "                    f\"**Model:** {model_info['name']}\\n\"\n",
        "                    f\"**Dataset:** {selected_dataset.upper()}\\n\"\n",
        "                    f\"**Accuracy:** {model_info['accuracy']:.2%}\"\n",
        "                )\n",
        "                \n",
        "                return result_text, sorted_probs, fig\n",
        "                \n",
        "            except Exception as e:\n",
        "                return f\"Error: {str(e)}\", {}, None\n",
        "        \n",
        "        def compare_all_models(image):\n",
        "            \"\"\"Compare all best models on same image\"\"\"\n",
        "            if image is None:\n",
        "                return pd.DataFrame(columns=[\"Dataset\", \"Model\", \"Prediction\", \"Confidence\", \"Accuracy\"])\n",
        "            \n",
        "            results = []\n",
        "            \n",
        "            for dataset, model_info in self.best_models.items():\n",
        "                try:\n",
        "                    # Preprocess for this dataset\n",
        "                    processed = self._preprocess_image(image, dataset)\n",
        "                    \n",
        "                    # Predict\n",
        "                    pred_labels, pred_probs = model_info['model'].predict(processed)\n",
        "                    \n",
        "                    # Get class name\n",
        "                    classes = self.prepared_data[dataset]['classes']\n",
        "                    pred_class = classes[pred_labels[0]]\n",
        "                    confidence = pred_probs[0][pred_labels[0]]\n",
        "                    \n",
        "                    results.append([\n",
        "                        dataset.upper(),\n",
        "                        model_info['name'],\n",
        "                        pred_class,\n",
        "                        f\"{confidence:.1%}\",\n",
        "                        f\"{model_info['accuracy']:.2%}\"\n",
        "                    ])\n",
        "                except Exception as e:\n",
        "                    results.append([dataset.upper(), model_info['name'], f\"Error: {str(e)}\", \"N/A\", \"N/A\"])\n",
        "            \n",
        "            return pd.DataFrame(results, columns=[\"Dataset\", \"Model\", \"Prediction\", \"Confidence\", \"Model Accuracy\"])\n",
        "        \n",
        "        # Create UI\n",
        "        with gr.Blocks(title=\"DNN Model Inference Dashboard\", theme=\"soft\") as demo:\n",
        "            gr.Markdown(\"# ðŸ§  DNN Model Inference Dashboard\")\n",
        "            gr.Markdown(\"Compare best models from our 3Ã—3 experiment matrix\")\n",
        "            \n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    dataset_dropdown = gr.Dropdown(\n",
        "                        choices=[\"mnist\", \"fashion\", \"cifar10\"],\n",
        "                        value=\"mnist\",\n",
        "                        label=\"Select Dataset\"\n",
        "                    )\n",
        "                    \n",
        "                    image_input = gr.Image(\n",
        "                        label=\"Upload or Draw Image\",\n",
        "                        type=\"numpy\",\n",
        "                        height=300,\n",
        "                        sources=[\"upload\", \"webcam\", \"canvas\"]\n",
        "                    )\n",
        "                    \n",
        "                    gr.Markdown(\"### Sample Images\")\n",
        "                    \n",
        "                    # Sample images for each dataset\n",
        "                    for dataset_name, samples in sample_images.items():\n",
        "                        gr.Markdown(f\"**{dataset_name.upper()}**\")\n",
        "                        with gr.Row():\n",
        "                            for label, img_array in samples:\n",
        "                                def make_click_handler(img=img_array):\n",
        "                                    return lambda: img\n",
        "                                \n",
        "                                gr.Button(label, size=\"sm\").click(\n",
        "                                    make_click_handler(),\n",
        "                                    outputs=image_input\n",
        "                                )\n",
        "                    \n",
        "                    predict_btn = gr.Button(\"Predict\", variant=\"primary\", size=\"lg\")\n",
        "                    compare_btn = gr.Button(\"Compare All Models\", variant=\"secondary\")\n",
        "                \n",
        "                with gr.Column(scale=2):\n",
        "                    result_text = gr.Markdown(\"## Prediction will appear here\")\n",
        "                    \n",
        "                    with gr.Row():\n",
        "                        with gr.Column(scale=1):\n",
        "                            prob_chart = gr.Plot(label=\"Probability Distribution\")\n",
        "                        with gr.Column(scale=1):\n",
        "                            prob_json = gr.JSON(label=\"Probabilities\")\n",
        "                    \n",
        "                    gr.Markdown(\"### Model Comparison\")\n",
        "                    comparison_table = gr.Dataframe(\n",
        "                        label=\"Comparison of Best Models\",\n",
        "                        interactive=False,\n",
        "                        wrap=True\n",
        "                    )\n",
        "                    \n",
        "                    gr.Markdown(\"### Model Information\")\n",
        "                    model_info = gr.JSON(\n",
        "                        value=self._get_model_info(),\n",
        "                        label=\"Available Models\"\n",
        "                    )\n",
        "            \n",
        "            # Connect events\n",
        "            predict_btn.click(\n",
        "                fn=predict,\n",
        "                inputs=[image_input, dataset_dropdown],\n",
        "                outputs=[result_text, prob_json, prob_chart]\n",
        "            )\n",
        "            \n",
        "            compare_btn.click(\n",
        "                fn=compare_all_models,\n",
        "                inputs=image_input,\n",
        "                outputs=comparison_table\n",
        "            )\n",
        "        \n",
        "        return demo\n",
        "    \n",
        "    def _prepare_sample_images(self):\n",
        "        \"\"\"Prepare sample images for each dataset\"\"\"\n",
        "        sample_images = {}\n",
        "        \n",
        "        for dataset_name in ['mnist', 'fashion', 'cifar10']:\n",
        "            # Get test data\n",
        "            X_test, y_test = self.prepared_data[dataset_name]['test']\n",
        "            classes = self.prepared_data[dataset_name]['classes']\n",
        "            \n",
        "            samples = []\n",
        "            # Get one sample per class\n",
        "            for class_idx in range(min(3, len(classes))):\n",
        "                class_indices = np.where(y_test == class_idx)[0]\n",
        "                if len(class_indices) > 0:\n",
        "                    idx = class_indices[0]\n",
        "                    img = X_test[idx]\n",
        "                    \n",
        "                    # Convert to uint8 for display\n",
        "                    if img.dtype == np.float32 or img.dtype == np.float64:\n",
        "                        img_display = (img * 255).astype(np.uint8)\n",
        "                    else:\n",
        "                        img_display = img.astype(np.uint8)\n",
        "                    \n",
        "                    label = f\"{dataset_name[:3]}_{classes[class_idx][:10]}\"\n",
        "                    samples.append((label, img_display))\n",
        "            \n",
        "            sample_images[dataset_name] = samples\n",
        "        \n",
        "        return sample_images\n",
        "    \n",
        "    def _preprocess_image(self, image, dataset_name):\n",
        "        \"\"\"Preprocess image for specific dataset\"\"\"\n",
        "        # Convert to numpy\n",
        "        if hasattr(image, 'shape'):\n",
        "            img_array = image\n",
        "        else:\n",
        "            img_array = np.array(image)\n",
        "        \n",
        "        # Handle RGBA\n",
        "        if img_array.ndim == 3 and img_array.shape[-1] == 4:\n",
        "            img_array = img_array[:, :, :3]\n",
        "        \n",
        "        # Resize\n",
        "        target_size = (28, 28) if dataset_name in ['mnist', 'fashion'] else (32, 32)\n",
        "        if img_array.shape[:2] != target_size:\n",
        "            img_pil = Image.fromarray(img_array.astype(np.uint8))\n",
        "            img_pil = img_pil.resize(target_size)\n",
        "            img_array = np.array(img_pil)\n",
        "        \n",
        "        # Convert to grayscale for MNIST/Fashion\n",
        "        if dataset_name in ['mnist', 'fashion'] and img_array.ndim == 3 and img_array.shape[-1] == 3:\n",
        "            img_array = np.mean(img_array, axis=-1, keepdims=False)\n",
        "        \n",
        "        # Add batch dimension\n",
        "        if img_array.ndim == 2:\n",
        "            img_array = img_array[np.newaxis, ...]\n",
        "        elif img_array.ndim == 3:\n",
        "            img_array = img_array[np.newaxis, ...]\n",
        "        \n",
        "        # Apply dataset-specific preprocessing\n",
        "        preprocessor = self.prepared_data[dataset_name]['preprocessor']\n",
        "        processed = preprocessor.transform(img_array)\n",
        "        \n",
        "        return processed\n",
        "    \n",
        "    def _create_probability_plot(self, probabilities, predicted_class, dataset_name):\n",
        "        \"\"\"Create probability bar chart\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        \n",
        "        classes = list(probabilities.keys())\n",
        "        probs = list(probabilities.values())\n",
        "        \n",
        "        # Colors: highlight predicted class\n",
        "        colors = ['lightblue' if cls != predicted_class else 'green' for cls in classes]\n",
        "        \n",
        "        bars = ax.bar(range(len(classes)), probs, color=colors, edgecolor='black')\n",
        "        \n",
        "        ax.set_xlabel('Classes')\n",
        "        ax.set_ylabel('Probability')\n",
        "        ax.set_title(f'Prediction Probabilities - {dataset_name.upper()}')\n",
        "        ax.set_xticks(range(len(classes)))\n",
        "        ax.set_xticklabels(classes, rotation=45, ha='right')\n",
        "        ax.set_ylim([0, 1.1])\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        # Add probability labels\n",
        "        for bar, prob in zip(bars, probs):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                   f'{prob:.1%}', ha='center', va='bottom', fontsize=8)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "    \n",
        "    def _get_model_info(self):\n",
        "        \"\"\"Get information about best models\"\"\"\n",
        "        info = {}\n",
        "        for dataset, model_info in self.best_models.items():\n",
        "            model = model_info['model']\n",
        "            info[dataset] = {\n",
        "                'model_name': model_info['name'],\n",
        "                'parameters': model.num_params,\n",
        "                'layers': len(model.layers),\n",
        "                'accuracy': f\"{model_info['accuracy']:.2%}\",\n",
        "                'architecture': model.layer_sizes\n",
        "            }\n",
        "        return info\n",
        "    \n",
        "    def launch(self, share=False):\n",
        "        \"\"\"Launch the UI\"\"\"\n",
        "        demo = self.create_ui()\n",
        "        \n",
        "        print(\"\\nðŸš€ Launching Gradio UI...\")\n",
        "        print(\"   Local URL: http://localhost:7860\")\n",
        "        if share:\n",
        "            print(\"   Public URL will be generated (expires in 72 hours)\")\n",
        "        \n",
        "        try:\n",
        "            demo.launch(share=share, server_name=\"0.0.0.0\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed to launch UI: {e}\")\n",
        "\n",
        "# Create and launch UI\n",
        "ui = ModelInferenceUI(all_models, prepared_data)\n",
        "\n",
        "print(\"\\nâœ… UI ready. Uncomment the line below to launch:\")\n",
        "print(\"# ui.launch(share=False)  # Set share=True for public URL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“‹ PART 9: Pipeline Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "import json\n",
        "\n",
        "print(\"\\nâœ… LECTURE 3 COMPLETE: SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save all results\n",
        "summary = {\n",
        "    \"experiment_matrix\": \"3 architectures Ã— 3 datasets = 9 models\",\n",
        "    \"total_models\": len(all_models),\n",
        "    \"datasets\": list(prepared_data.keys()),\n",
        "    \"architectures\": list(ARCHITECTURES.keys()),\n",
        "    \"best_models\": {},\n",
        "    \"dataset_stats\": dataset_stats,\n",
        "    \"total_training_time\": sum(sum(m.history['epoch_times']) for m in all_models.values()),\n",
        "    \"total_parameters\": sum(m.num_params for m in all_models.values()),\n",
        "    \"pipeline_created\": True,\n",
        "    \"visualizations_created\": len(list(VIZ_DIR.glob(\"*.png\"))),\n",
        "    \"models_saved\": len(list(MODELS_DIR.glob(\"*.pkl\"))),\n",
        "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "}\n",
        "\n",
        "# Add best models\n",
        "for dataset, model_info in ui.best_models.items():\n",
        "    summary[\"best_models\"][dataset] = {\n",
        "        \"model_name\": model_info[\"name\"],\n",
        "        \"accuracy\": float(model_info[\"accuracy\"]),\n",
        "        \"parameters\": int(model_info[\"model\"].num_params)\n",
        "    }\n",
        "\n",
        "# Save summary\n",
        "summary_path = RESULTS_DIR / \"lecture3_summary.json\"\n",
        "with open(summary_path, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nðŸ“Š EXPERIMENT SUMMARY:\")\n",
        "print(f\"   â€¢ 9 models trained (3 architectures Ã— 3 datasets)\")\n",
        "print(f\"   â€¢ Total parameters: {summary['total_parameters']:,}\")\n",
        "print(f\"   â€¢ Total training time: {summary['total_training_time']:.0f}s\")\n",
        "print(f\"   â€¢ Models saved to: {MODELS_DIR}\")\n",
        "print(f\"   â€¢ Visualizations saved to: {VIZ_DIR}\")\n",
        "print(f\"   â€¢ Summary saved to: {summary_path}\")\n",
        "\n",
        "print(\"\\nðŸ† BEST MODELS PER DATASET:\")\n",
        "for dataset, model_info in ui.best_models.items():\n",
        "    print(f\"   â€¢ {dataset.upper():<12} â†’ {model_info['name']:<25} ({model_info['accuracy']:.2%})\")\n",
        "\n",
        "print(\"\\nðŸ”œ NEXT STEPS:\")\n",
        "print(\"   1. Uncomment UI launch to interact with models\")\n",
        "print(\"   2. Analyze where DNNs fail (spatial patterns, translations)\")\n",
        "print(\"   3. Prepare for Lecture 4: Building CNNs from scratch\")\n",
        "print(\"   4. Convert this notebook to production pipeline (see structure below)\")\n",
        "\n",
        "print(\"\\nðŸ—ï¸ PRODUCTION PIPELINE STRUCTURE:\")\n",
        "print(\"config/           # YAML/JSON config files\")\n",
        "print(\"src/              # Source code\")\n",
        "print(\"  data/           # Data loading & preprocessing\")\n",
        "print(\"  models/         # Model definitions\")\n",
        "print(\"  training/       # Training loops\")\n",
        "print(\"  evaluation/     # Metrics & visualization\")\n",
        "print(\"  inference/      # Prediction & serving\")\n",
        "print(\"scripts/          # CLI entry points\")\n",
        "print(\"tests/            # Unit tests\")\n",
        "print(\"experiments/      # Experiment tracking\")\n",
        "print(\"models/           # Saved models\")\n",
        "print(\"results/          # Results & logs\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ KEY ACHIEVEMENTS:\")\n",
        "print(\"1. Loaded 3 datasets directly from URLs (no libraries)\")\n",
        "print(\"2. Trained 9 models in 3Ã—3 experiment matrix\")\n",
        "print(\"3. Visualized training dynamics for all models\")\n",
        "print(\"4. Demonstrated DNN limitations with complex data\")\n",
        "print(\"5. Designed production-ready pipeline architecture\")\n",
        "print(\"6. Built interactive UI for model comparison\")\n",
        "print(\"7. Prepared clear path to CNNs (next lecture)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸŽ‰ LECTURE 3 COMPLETE! Ready for CNNs in Lecture 4! ðŸŽ‰\")\n",
        "print(\"=\" * 80)"
      ]
    }
  ]
}