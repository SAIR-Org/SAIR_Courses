{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö PyTorch Practice Notebook - Lecture 2: Professional Data Pipelines\n",
    "\n",
    "**Based on:** SAIR PyTorch Mastery - Lecture 2: Professional Data Pipelines with PyTorch\n",
    "\n",
    "**Instructions:** Complete the exercises below to test your understanding of PyTorch data pipelines. Try to solve them without looking at the original notebook first!\n",
    "\n",
    "**Time Estimate:** 3-4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup & Imports\n",
    "\n",
    "Run this cell first to set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import tempfile\n",
    "import shutil\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 1: Dataset Fundamentals & Memory Management\n",
    "\n",
    "### Part A: Fix the Memory-Inefficient Dataset\n",
    "\n",
    "**Task:** This dataset loads ALL data in `__init__`, which is inefficient for large datasets. Rewrite it to use lazy loading.\n",
    "\n",
    "**Original (problematic) implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== PROBLEMATIC DATASET - FIX ME! ===========\n",
    "class MemoryInefficientDataset(Dataset):\n",
    "    \"\"\"Dataset that loads ALL data in __init__ - problematic for large datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__()\n",
    "        # PROBLEM: Loading ALL data at initialization\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # PROBLEM: Converting ALL to tensors upfront\n",
    "        self.features = torch.tensor(self.data.iloc[:, :-1].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(self.data.iloc[:, -1].values, dtype=torch.float32)\n",
    "        \n",
    "        print(f\"Loaded {len(self)} samples\")\n",
    "        print(f\"Memory usage: {self.features.element_size() * self.features.nelement() / 1e6:.1f} MB for features\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # No actual loading needed - data already in memory\n",
    "        return self.features[idx], self.labels[idx]\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task:** Rewrite the dataset to:\n",
    "1. Load only metadata in `__init__`\n",
    "2. Load data on-demand in `__getitem__`\n",
    "3. Handle CSV files larger than memory\n",
    "\n",
    "**Test Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test CSV data\n",
    "test_csv_data = \"\"\"feature1,feature2,feature3,feature4,label\n",
    "1.2,3.4,5.6,7.8,0\n",
    "2.3,4.5,6.7,8.9,1\n",
    "3.4,5.6,7.8,9.0,0\n",
    "4.5,6.7,8.9,10.1,1\n",
    "5.6,7.8,9.0,11.2,0\n",
    "\"\"\"\n",
    "\n",
    "# Save to temporary file\n",
    "temp_csv = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
    "temp_csv.write(test_csv_data)\n",
    "temp_csv.close()\n",
    "\n",
    "print(f\"Test CSV created at: {temp_csv.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "class MemoryEfficientDataset(Dataset):\n",
    "    \"\"\"Your optimized dataset implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__()\n",
    "        # TODO: Load only metadata, not data\n",
    "        \n",
    "    def __len__(self):\n",
    "        # TODO: Return dataset length\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Load data on-demand\n",
    "        pass\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Memory Usage Comparison\n",
    "\n",
    "**Task:** Compare memory usage between the two implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "# 1. Test memory inefficient version\n",
    "print(\"Testing Memory Inefficient Dataset:\")\n",
    "# TODO: Instantiate and measure memory\n",
    "\n",
    "# 2. Test your memory efficient version\n",
    "print(\"\\nTesting Your Memory Efficient Dataset:\")\n",
    "# TODO: Instantiate and measure memory\n",
    "\n",
    "# 3. Load a few samples and measure time\n",
    "print(\"\\nTesting sample loading performance:\")\n",
    "# TODO: Time loading of 1000 samples for each\n",
    "\n",
    "# 4. Cleanup\n",
    "os.unlink(temp_csv.name)\n",
    "print(f\"Cleaned up temporary file\")\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Exercise 2: DataLoader Optimization\n",
    "\n",
    "### Part A: Diagnose and Fix Slow Data Loading\n",
    "\n",
    "**Task:** This training script has slow data loading. Identify the bottlenecks and fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== SLOW TRAINING SCRIPT - FIX ME! ===========\n",
    "class SlowDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000):\n",
    "        self.num_samples = num_samples\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Simulate slow image generation\n",
    "        for i in range(num_samples):\n",
    "            # Simulate image loading/processing\n",
    "            time.sleep(0.001)  # 1ms delay per image\n",
    "            self.images.append(torch.randn(3, 224, 224))\n",
    "            self.labels.append(i % 10)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Slow training setup\n",
    "dataset = SlowDataset(num_samples=100)\n",
    "\n",
    "# PROBLEMATIC DataLoader configuration\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # PROBLEM: No parallel loading\n",
    "    pin_memory=False,  # PROBLEM: Not using pinned memory for GPU\n",
    ")\n",
    "\n",
    "# Simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16, 10)\n",
    ").to(device)\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Starting slow training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(2):\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"Total training time: {time.time() - start_time:.2f} seconds\")\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task:**\n",
    "1. Identify all bottlenecks in the code above\n",
    "2. Rewrite the dataset to be more efficient\n",
    "3. Optimize the DataLoader configuration\n",
    "4. Show performance improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR OPTIMIZED SOLUTION ===========\n",
    "# 1. Create an optimized dataset\n",
    "class OptimizedDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000):\n",
    "        # TODO: Optimize initialization\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Optimize data loading\n",
    "        pass\n",
    "\n",
    "# 2. Create optimized DataLoader\n",
    "# TODO: Choose optimal parameters\n",
    "# dataloader_optimized = DataLoader(...)\n",
    "\n",
    "# 3. Benchmark performance\n",
    "print(\"\\nBenchmarking Optimized Version:\")\n",
    "# TODO: Run training with optimized setup and measure time\n",
    "\n",
    "# 4. Compare performance\n",
    "# TODO: Show speedup factor\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Profile Data Loading Performance\n",
    "\n",
    "**Task:** Create a profiling tool that measures:\n",
    "1. Batch loading times\n",
    "2. GPU idle time\n",
    "3. Memory usage during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "class DataLoaderProfiler:\n",
    "    \"\"\"Your implementation of a data loading profiler\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'batch_times': [],\n",
    "            'gpu_idle_times': [],\n",
    "            'memory_usage': [],\n",
    "            'cpu_usage': []\n",
    "        }\n",
    "    \n",
    "    def profile_training(self, model, dataloader, num_batches=20):\n",
    "        \"\"\"Profile training loop\"\"\"\n",
    "        # TODO: Implement profiling\n",
    "        pass\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print profiling results\"\"\"\n",
    "        # TODO: Print detailed report\n",
    "        pass\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Visualize metrics\"\"\"\n",
    "        # TODO: Create plots\n",
    "        pass\n",
    "\n",
    "# Test your profiler\n",
    "profiler = DataLoaderProfiler()\n",
    "# TODO: Profile both slow and optimized versions\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Exercise 3: Computer Vision Pipeline\n",
    "\n",
    "### Part A: Create Augmentation Pipeline for Sudanese Agriculture\n",
    "\n",
    "**Task:** Design data augmentations specifically for Sudanese agricultural images.\n",
    "\n",
    "**Considerations:**\n",
    "- Plants might be at different angles\n",
    "- Varying lighting conditions (bright sun vs shade)\n",
    "- Dust/sand particles in air\n",
    "- Different camera angles (from drone vs ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "# 1. Create training augmentations\n",
    "sudanese_agriculture_train_transform = transforms.Compose([\n",
    "    # TODO: Design appropriate augmentations\n",
    "    # Consider: rotation, color jitter, random crop, etc.\n",
    "])\n",
    "\n",
    "# 2. Create validation augmentations (simpler)\n",
    "sudanese_agriculture_val_transform = transforms.Compose([\n",
    "    # TODO: Simple preprocessing for validation\n",
    "])\n",
    "\n",
    "# 3. Create test function\n",
    "def test_augmentations(transform, num_samples=4):\n",
    "    \"\"\"Test and visualize augmentations\"\"\"\n",
    "    # TODO: Create dummy image and apply transformations\n",
    "    # Visualize original + augmented versions\n",
    "    pass\n",
    "\n",
    "# Test your augmentations\n",
    "print(\"Testing Sudanese Agriculture Augmentations:\")\n",
    "test_augmentations(sudanese_agriculture_train_transform)\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Handle Large Satellite Images\n",
    "\n",
    "**Task:** Create a dataset that can handle very large satellite images (e.g., 10,000√ó10,000 pixels) without loading them entirely into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "class SatelliteImageDataset(Dataset):\n",
    "    \"\"\"Dataset for large satellite images using tiling\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, tile_size=512, overlap=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths: List of paths to large satellite images\n",
    "            labels: List of labels (e.g., crop type, drought level)\n",
    "            tile_size: Size of tiles to extract\n",
    "            overlap: Overlap between tiles to avoid edge artifacts\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.tile_size = tile_size\n",
    "        self.overlap = overlap\n",
    "        \n",
    "        # TODO: Pre-calculate tile information\n",
    "        # Store tile metadata (image_idx, x, y, label)\n",
    "        self.tiles = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        # TODO: Return number of tiles\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Load only the needed tile from large image\n",
    "        pass\n",
    "    \n",
    "    def visualize_tile(self, idx, show_grid=True):\n",
    "        \"\"\"Visualize a tile within the context of the full image\"\"\"\n",
    "        # TODO: Implement visualization\n",
    "        pass\n",
    "\n",
    "# Test with simulated large images\n",
    "print(\"Creating test satellite images...\")\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "\n",
    "# TODO: Create test images and test your dataset\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree(temp_dir)\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Exercise 4: NLP Pipeline for Arabic Text\n",
    "\n",
    "### Part A: Handle Sudanese Arabic Dialect\n",
    "\n",
    "**Task:** Create a text dataset that handles Sudanese Arabic dialect features:\n",
    "1. Right-to-left text\n",
    "2. Dialect-specific words\n",
    "3. Handle both Modern Standard Arabic and Sudanese dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "class SudaneseArabicDataset(Dataset):\n",
    "    \"\"\"Dataset for Sudanese Arabic text classification\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, vocab=None, max_length=128, \n",
    "                 handle_dialect=True, normalize=True):\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "        self.handle_dialect = handle_dialect\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # TODO: Build vocabulary considering dialect\n",
    "        if vocab is None:\n",
    "            self.vocab = self._build_vocab(texts)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        \n",
    "        # TODO: Add special tokens\n",
    "        \n",
    "    def _build_vocab(self, texts):\n",
    "        \"\"\"Build vocabulary with dialect handling\"\"\"\n",
    "        # TODO: Implement vocabulary building\n",
    "        # Consider: dialect normalization, MSA mapping, etc.\n",
    "        pass\n",
    "    \n",
    "    def _preprocess_text(self, text):\n",
    "        \"\"\"Preprocess Arabic text\"\"\"\n",
    "        # TODO: Implement preprocessing steps:\n",
    "        # 1. Normalize Arabic characters\n",
    "        # 2. Remove diacritics (optional)\n",
    "        # 3. Handle dialect words (map to MSA or keep)\n",
    "        # 4. Other cleaning steps\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Implement text encoding\n",
    "        pass\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Convert token IDs back to text\"\"\"\n",
    "        # TODO: Implement decoding\n",
    "        pass\n",
    "\n",
    "# Test data\n",
    "sudanese_texts = [\n",
    "    \"ŸÉŸêÿ≥ÿ±ÿ© ÿ®ÿ™ÿßÿπÿ© ŸÅŸàŸÑ ŸÖÿπ ÿ∑ŸÖÿßÿ∑ŸÖ\",  # Sudanese dialect\n",
    "    \"ÿßŸÑÿ∑ŸÇÿ≥ ÿßŸÑŸäŸàŸÖ ÿ≠ÿßÿ± ÿ¨ÿØÿßŸã\",  # Modern Standard Arabic\n",
    "    \"ÿ¥ÿßŸäŸÅ ÿßŸÑŸÇŸàŸÖ ÿØŸá ÿπÿßŸÖŸÑŸäŸÜ ÿ•ÿ≤ÿßŸä\",  # Sudanese dialect\n",
    "    \"ÿßŸÑÿ≤ÿ±ÿßÿπÿ© ŸÅŸä ÿßŸÑÿ≥ŸàÿØÿßŸÜ ŸÖÿ™ŸÇÿØŸÖÿ©\",  # MSA\n",
    "    \"ÿπÿßŸäÿ≤ ÿ£ÿ¥ŸàŸÅ ŸÉŸÖÿßÿ¥ÿ©\",  # Sudanese dialect\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0]  # 0 = dialect, 1 = MSA\n",
    "\n",
    "# Test your dataset\n",
    "print(\"Testing Sudanese Arabic Dataset:\")\n",
    "dataset = SudaneseArabicDataset(sudanese_texts, labels)\n",
    "# TODO: Test encoding/decoding\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Streaming Dataset for Large Text Corpora\n",
    "\n",
    "**Task:** Create a streaming dataset that can handle text files larger than memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "class StreamingArabicNews(IterableDataset):\n",
    "    \"\"\"Streaming dataset for Arabic news articles\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, vocab=None, max_length=128, \n",
    "                 buffer_size=1000, shuffle=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_path: Path to large text file (one article per line)\n",
    "            vocab: Pre-built vocabulary\n",
    "            max_length: Maximum sequence length\n",
    "            buffer_size: Number of lines to buffer\n",
    "            shuffle: Whether to shuffle the stream\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.vocab = vocab or self._build_vocab_from_file()\n",
    "        self.max_length = max_length\n",
    "        self.buffer_size = buffer_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # TODO: Initialize\n",
    "    \n",
    "    def _build_vocab_from_file(self):\n",
    "        \"\"\"Build vocabulary by streaming through file once\"\"\"\n",
    "        # TODO: Implement vocabulary building from stream\n",
    "        pass\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Stream data from file\"\"\"\n",
    "        # TODO: Implement streaming logic\n",
    "        # Consider: worker splitting, buffering, shuffling\n",
    "        pass\n",
    "\n",
    "# Create test large text file\n",
    "print(\"Creating test text file...\")\n",
    "temp_text_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8')\n",
    "\n",
    "# Generate Arabic text\n",
    "arabic_samples = [\n",
    "    \"ÿ™ŸÇÿ±Ÿäÿ± ÿπŸÜ ÿßŸÑÿ≤ÿ±ÿßÿπÿ© ŸÅŸä ÿßŸÑÿ≥ŸàÿØÿßŸÜ\",\n",
    "    \"ÿ£ÿÆÿ®ÿßÿ± ÿßŸÑÿ±Ÿäÿßÿ∂ÿ© ÿßŸÑŸÖÿ≠ŸÑŸäÿ©\",\n",
    "    \"ÿ™ÿ∑Ÿàÿ±ÿßÿ™ ÿßŸÑÿ≥ŸàŸÇ ÿßŸÑŸÖÿßŸÑŸäÿ©\",\n",
    "    \"ÿßŸÑÿ∑ŸÇÿ≥ Ÿàÿ£ÿ≠ŸàÿßŸÑ ÿßŸÑÿ≤ÿ±ÿßÿπÿ©\",\n",
    "    \"ÿßŸÑÿ™ÿπŸÑŸäŸÖ ŸÅŸä ÿßŸÑŸÖŸÜÿßÿ∑ŸÇ ÿßŸÑÿ±ŸäŸÅŸäÿ©\",\n",
    "]\n",
    "\n",
    "# Write many lines to simulate large file\n",
    "for i in range(100):\n",
    "    for sample in arabic_samples:\n",
    "        temp_text_file.write(f\"{sample} - ÿßŸÑŸÜÿ≥ÿÆÿ© {i}\\n\")\n",
    "temp_text_file.close()\n",
    "\n",
    "print(f\"Test file created: {temp_text_file.name} ({os.path.getsize(temp_text_file.name)} bytes)\")\n",
    "\n",
    "# Test your streaming dataset\n",
    "print(\"\\nTesting Streaming Dataset:\")\n",
    "# TODO: Test the streaming dataset\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(temp_text_file.name)\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Challenge Problems\n",
    "\n",
    "### Challenge 1: Multi-Modal Dataset (Images + Text)\n",
    "\n",
    "**Task:** Create a dataset that handles both images and text for a Sudanese market monitoring system.\n",
    "\n",
    "**Scenario:** You're building a system that:\n",
    "- Takes photos of market goods (sorghum, millet, wheat)\n",
    "- Has Arabic text descriptions from sellers\n",
    "- Includes price information\n",
    "- Needs to predict whether prices are reasonable\n",
    "\n",
    "**Requirements:**\n",
    "1. Handle image loading and augmentation\n",
    "2. Process Arabic text descriptions\n",
    "3. Combine multiple data types in single sample\n",
    "4. Handle missing data (some samples might have only image or only text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== CHALLENGE 1 ===========\n",
    "class SudaneseMarketMultiModalDataset(Dataset):\n",
    "    \"\"\"Multi-modal dataset for Sudanese market monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 image_dir,  # Directory with images\n",
    "                 metadata_file,  # CSV with text, prices, etc.\n",
    "                 image_transform=None,\n",
    "                 text_max_length=50,\n",
    "                 handle_missing='zero'):  # How to handle missing data\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir: Directory containing product images\n",
    "            metadata_file: CSV with columns: image_name, description, price, category, is_reasonable\n",
    "            image_transform: Transformations for images\n",
    "            text_max_length: Maximum text sequence length\n",
    "            handle_missing: Strategy for missing data ('zero', 'mean', 'ignore')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_dir = Path(image_dir)\n",
    "        \n",
    "        # TODO: Load metadata\n",
    "        self.metadata = pd.read_csv(metadata_file)\n",
    "        \n",
    "        # TODO: Initialize image transformations\n",
    "        self.image_transform = image_transform or self._default_image_transform()\n",
    "        \n",
    "        # TODO: Initialize text processing\n",
    "        self.text_max_length = text_max_length\n",
    "        self.text_vocab = self._build_text_vocab()\n",
    "        \n",
    "        # TODO: Handle missing data strategy\n",
    "        self.handle_missing = handle_missing\n",
    "        \n",
    "        # TODO: Preprocess data\n",
    "        \n",
    "    def _default_image_transform(self):\n",
    "        \"\"\"Default image transformations for market products\"\"\"\n",
    "        # TODO: Design appropriate transformations\n",
    "        pass\n",
    "    \n",
    "    def _build_text_vocab(self):\n",
    "        \"\"\"Build vocabulary from Arabic descriptions\"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return multi-modal sample\"\"\"\n",
    "        row = self.metadata.iloc[idx]\n",
    "        \n",
    "        # TODO: Load and process image\n",
    "        image_tensor = None  # Load image if exists\n",
    "        \n",
    "        # TODO: Process Arabic text\n",
    "        text_tensor = None  # Encode text if exists\n",
    "        \n",
    "        # TODO: Handle price/numerical features\n",
    "        price_tensor = None\n",
    "        \n",
    "        # TODO: Handle missing data\n",
    "        \n",
    "        # TODO: Combine into single sample\n",
    "        # Sample structure:\n",
    "        # {\n",
    "        #     'image': image_tensor,\n",
    "        #     'text': text_tensor,\n",
    "        #     'price': price_tensor,\n",
    "        #     'label': row['is_reasonable']  # 0 or 1\n",
    "        # }\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Custom collate function for multi-modal data\"\"\"\n",
    "        # TODO: Implement collate function that handles:\n",
    "        # - Variable length sequences\n",
    "        # - Missing modalities\n",
    "        # - Different data types\n",
    "        pass\n",
    "\n",
    "# Create test data\n",
    "print(\"Creating test multi-modal data...\")\n",
    "temp_mm_dir = Path(tempfile.mkdtemp())\n",
    "\n",
    "# TODO: Create test images and metadata\n",
    "# 1. Create image directory with dummy images\n",
    "# 2. Create CSV metadata file\n",
    "\n",
    "# Test your dataset\n",
    "print(\"\\nTesting Multi-Modal Dataset:\")\n",
    "# dataset = SudaneseMarketMultiModalDataset(...)\n",
    "# dataloader = DataLoader(dataset, batch_size=4, collate_fn=dataset.collate_fn)\n",
    "# TODO: Test batch loading\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree(temp_mm_dir)\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Data Pipeline for Sudanese Healthcare\n",
    "\n",
    "**Task:** Design a complete data pipeline for medical imaging in Sudanese hospitals.\n",
    "\n",
    "**Special Considerations:**\n",
    "1. Handle DICOM files (medical images)\n",
    "2. Include patient metadata\n",
    "3. Respect patient privacy (anonymization)\n",
    "4. Work with limited internet connectivity (offline capable)\n",
    "5. Handle power outages (checkpointing)\n",
    "\n",
    "**Bonus:** Implement data validation to catch corrupted files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== CHALLENGE 2 ===========\n",
    "class SudaneseHealthcareDataset(Dataset):\n",
    "    \"\"\"Dataset for Sudanese healthcare applications\"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, transform=None, anonymize=True,\n",
    "                 validate_data=True, cache_size=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_root: Root directory with structure:\n",
    "                - images/ (DICOM or PNG files)\n",
    "                - metadata.csv (patient info, diagnoses)\n",
    "                - annotations/ (optional: segmentation masks)\n",
    "            transform: Image transformations\n",
    "            anonymize: Whether to anonymize patient data\n",
    "            validate_data: Validate file integrity\n",
    "            cache_size: Number of samples to cache in memory\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_root = Path(data_root)\n",
    "        \n",
    "        # TODO: Implement with healthcare-specific considerations\n",
    "        \n",
    "    def _load_and_validate_dicom(self, filepath):\n",
    "        \"\"\"Load and validate DICOM file\"\"\"\n",
    "        # TODO: Implement DICOM loading with validation\n",
    "        pass\n",
    "    \n",
    "    def _anonymize_metadata(self, metadata):\n",
    "        \"\"\"Remove personally identifiable information\"\"\"\n",
    "        # TODO: Implement anonymization\n",
    "        pass\n",
    "    \n",
    "    def _checkpoint_state(self):\n",
    "        \"\"\"Save dataset state for recovery from power outages\"\"\"\n",
    "        # TODO: Implement checkpointing\n",
    "        pass\n",
    "    \n",
    "    def _restore_from_checkpoint(self):\n",
    "        \"\"\"Restore dataset state\"\"\"\n",
    "        # TODO: Implement restoration\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "# Create a DataValidator class\n",
    "class HealthcareDataValidator:\n",
    "    \"\"\"Validate healthcare data integrity\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_dicom(filepath):\n",
    "        \"\"\"Validate DICOM file integrity\"\"\"\n",
    "        # TODO: Check if DICOM is valid and not corrupted\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_metadata(metadata):\n",
    "        \"\"\"Validate patient metadata\"\"\"\n",
    "        # TODO: Check required fields, data types, ranges\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_anonymization(metadata):\n",
    "        \"\"\"Check if data is properly anonymized\"\"\"\n",
    "        # TODO: Verify no PII remains\n",
    "        pass\n",
    "\n",
    "# Design document\n",
    "print(\"\"\"\n",
    "Design Considerations for Sudanese Healthcare Pipeline:\n",
    "\n",
    "1. OFFLINE OPERATION:\n",
    "   - Local caching of all data\n",
    "   - Pre-processed datasets stored locally\n",
    "   - Batch processing for when connectivity is available\n",
    "\n",
    "2. POWER RESILIENCE:\n",
    "   - Regular checkpointing of dataset state\n",
    "   - Incremental processing with recovery\n",
    "   - Battery backup considerations\n",
    "\n",
    "3. PRIVACY:\n",
    "   - Automatic anonymization of patient data\n",
    "   - Encryption of sensitive data\n",
    "   - Access controls\n",
    "\n",
    "4. VALIDATION:\n",
    "   - File integrity checks\n",
    "   - Data completeness validation\n",
    "   - Cross-field consistency checks\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Implement test cases for your design\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Assessment Questions\n",
    "\n",
    "Answer these questions in markdown cells:\n",
    "\n",
    "### Q1: When should you use `num_workers=0` in DataLoader? What are the trade-offs?\n",
    "\n",
    "### Q2: What's the difference between `pin_memory=True` and `pin_memory=False`? When would you use each?\n",
    "\n",
    "### Q3: How does `prefetch_factor` affect performance and memory usage?\n",
    "\n",
    "### Q4: What are the main differences between `Dataset` and `IterableDataset`? Give examples of when to use each.\n",
    "\n",
    "### Q5: How would you handle a dataset where some samples have corrupted files?\n",
    "\n",
    "### Q6: What special considerations are needed for Arabic text processing vs English?\n",
    "\n",
    "### Q7: How would you design a data pipeline that works in areas with intermittent internet connectivity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Progress Tracker\n",
    "\n",
    "Check off exercises as you complete them:\n",
    "\n",
    "- [ ] Exercise 1A: Fix Memory-Inefficient Dataset\n",
    "- [ ] Exercise 1B: Memory Usage Comparison\n",
    "- [ ] Exercise 2A: Diagnose & Fix Slow Data Loading\n",
    "- [ ] Exercise 2B: Profile Data Loading Performance\n",
    "- [ ] Exercise 3A: Sudanese Agriculture Augmentations\n",
    "- [ ] Exercise 3B: Large Satellite Images Dataset\n",
    "- [ ] Exercise 4A: Sudanese Arabic Dialect Dataset\n",
    "- [ ] Exercise 4B: Streaming Text Dataset\n",
    "- [ ] Challenge 1: Multi-Modal Dataset (Images + Text)\n",
    "- [ ] Challenge 2: Sudanese Healthcare Pipeline\n",
    "- [ ] Assessment Questions Q1-Q7\n",
    "\n",
    "## üèÜ Completion Certificate\n",
    "\n",
    "Once you complete all exercises, you've mastered:\n",
    "- ‚úÖ PyTorch Dataset design patterns\n",
    "- ‚úÖ DataLoader optimization techniques\n",
    "- ‚úÖ Computer vision pipelines with augmentation\n",
    "- ‚úÖ NLP pipelines for Arabic text\n",
    "- ‚úÖ Multi-modal data handling\n",
    "- ‚úÖ Production considerations for Sudanese context\n",
    "\n",
    "**You're ready for Lecture 3: Advanced Model Architectures & Training!** üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Tips for Success\n",
    "\n",
    "1. **Start Simple**: Begin with basic implementations, then optimize\n",
    "2. **Profile Early**: Use the profiler to identify bottlenecks\n",
    "3. **Test with Small Data**: Verify correctness before scaling up\n",
    "4. **Consider Sudanese Context**: Think about real-world constraints\n",
    "5. **Document Your Choices**: Explain why you made certain design decisions\n",
    "\n",
    "## ü§ù Need Help?\n",
    "\n",
    "- Review Lecture 2 notebook for concepts\n",
    "- Use PyTorch documentation for specific APIs\n",
    "- Test your implementations step by step\n",
    "- Consider edge cases (missing data, large files, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
