{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š PyTorch Practice Notebook - Lecture 2: Professional Data Pipelines\n",
    "\n",
    "**Based on:** SAIR PyTorch Mastery - Lecture 2: Professional Data Pipelines with PyTorch\n",
    "\n",
    "**Instructions:** Complete the exercises below to test your understanding of PyTorch data pipelines. Try to solve them without looking at the original notebook first!\n",
    "\n",
    "**Time Estimate:** 3-4 hours\n",
    "\n",
    "## ðŸ†• Enhanced Features:\n",
    "- Edge case testing (corrupt files, missing data)\n",
    "- Performance comparison exercises\n",
    "- Debugging exercises (finding bugs in given code)\n",
    "- Additional Sudanese context scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup & Imports\n",
    "\n",
    "Run this cell first to set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import tempfile\n",
    "import shutil\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ†• NEW: Debugging Exercise 0 - Find the Bugs!\n",
    "\n",
    "**Task:** This dataset class has multiple bugs. Identify and fix them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== BUGGY DATASET - FIND AND FIX ALL BUGS! ===========\n",
    "class BuggyImageDataset(Dataset):\n",
    "    \"\"\"Dataset with multiple bugs - fix them all!\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, label_file):\n",
    "        # BUG 1: Missing super().__init__()\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        \n",
    "        # BUG 2: No error handling for missing file\n",
    "        self.labels = pd.read_csv(label_file)\n",
    "        \n",
    "        # BUG 3: Inefficient - loading all image paths upfront\n",
    "        self.image_paths = []\n",
    "        for ext in ['.jpg', '.png', '.jpeg']:\n",
    "            self.image_paths.extend(list(Path(image_dir).glob(f'*{ext}')))\n",
    "            \n",
    "        # BUG 4: No validation that images match labels\n",
    "        \n",
    "        # BUG 5: Transform applied differently each time\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        # BUG 6: Inconsistent length\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # BUG 7: No error handling for corrupt files\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        # BUG 8: Opening image but not closing it properly\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        # BUG 9: Applying random transform differently each time\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # BUG 10: Hardcoded label extraction\n",
    "        label = self.labels.iloc[idx]['label']\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def show_sample(self, idx):\n",
    "        # BUG 11: Modifies the image but doesn't return it\n",
    "        img, label = self[idx]\n",
    "        plt.imshow(img.permute(1, 2, 0) if len(img.shape) == 3 else img)\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.show()\n",
    "\n",
    "# =========== YOUR FIXED VERSION ===========\n",
    "class FixedImageDataset(Dataset):\n",
    "    \"\"\"Your fixed version of the buggy dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, label_file):\n",
    "        # TODO: Fix all bugs\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "    \n",
    "    def show_sample(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Exercise 1: Dataset Fundamentals & Memory Management\n",
    "\n",
    "### Part A: Fix the Memory-Inefficient Dataset\n",
    "\n",
    "**Task:** This dataset loads ALL data in `__init__`, which is inefficient for large datasets. Rewrite it to use lazy loading.\n",
    "\n",
    "**Original (problematic) implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== PROBLEMATIC DATASET - FIX ME! ===========\n",
    "class MemoryInefficientDataset(Dataset):\n",
    "    \"\"\"Dataset that loads ALL data in __init__ - problematic for large datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__()\n",
    "        # PROBLEM: Loading ALL data at initialization\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        \n",
    "        # PROBLEM: Converting ALL to tensors upfront\n",
    "        self.features = torch.tensor(self.data.iloc[:, :-1].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(self.data.iloc[:, -1].values, dtype=torch.float32)\n",
    "        \n",
    "        print(f\"Loaded {len(self)} samples\")\n",
    "        print(f\"Memory usage: {self.features.element_size() * self.features.nelement() / 1e6:.1f} MB for features\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # No actual loading needed - data already in memory\n",
    "        return self.features[idx], self.labels[idx]\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task:** Rewrite the dataset to:\n",
    "1. Load only metadata in `__init__`\n",
    "2. Load data on-demand in `__getitem__`\n",
    "3. Handle CSV files larger than memory\n",
    "\n",
    "**Test Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test CSV data\n",
    "test_csv_data = \"\"\"feature1,feature2,feature3,feature4,label\n",
    "1.2,3.4,5.6,7.8,0\n",
    "2.3,4.5,6.7,8.9,1\n",
    "3.4,5.6,7.8,9.0,0\n",
    "4.5,6.7,8.9,10.1,1\n",
    "5.6,7.8,9.0,11.2,0\n",
    "\"\"\"\n",
    "\n",
    "# Save to temporary file\n",
    "temp_csv = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
    "temp_csv.write(test_csv_data)\n",
    "temp_csv.close()\n",
    "\n",
    "print(f\"Test CSV created at: {temp_csv.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "class MemoryEfficientDataset(Dataset):\n",
    "    \"\"\"Your optimized dataset implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__()\n",
    "        # TODO: Load only metadata, not data\n",
    "        \n",
    "    def __len__(self):\n",
    "        # TODO: Return dataset length\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Load data on-demand\n",
    "        pass\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ†• NEW: Part A-2: Handle Corrupt/Missing Data\n",
    "\n",
    "**Task:** Extend your dataset to handle:\n",
    "1. Corrupted rows in CSV\n",
    "2. Missing values (NaN)\n",
    "3. Invalid data types\n",
    "\n",
    "**Test with corrupt data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV with corrupt/missing data\n",
    "corrupt_csv_data = \"\"\"feature1,feature2,feature3,feature4,label\n",
    "1.2,3.4,5.6,7.8,0\n",
    "2.3,4.5,corrupt,8.9,1  # String where float expected\n",
    "3.4,5.6,7.8,,0  # Missing value\n",
    "4.5,6.7,8.9,10.1,1\n",
    "invalid_row_with_extra_columns,1,2,3,4,5,6  # Wrong number of columns\n",
    "5.6,7.8,9.0,11.2,0\n",
    "\"\"\"\n",
    "\n",
    "corrupt_csv = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
    "corrupt_csv.write(corrupt_csv_data)\n",
    "corrupt_csv.close()\n",
    "\n",
    "print(f\"Corrupt test CSV created at: {corrupt_csv.name}\")\n",
    "\n",
    "# =========== YOUR CODE HERE ===========\n",
    "class RobustMemoryEfficientDataset(Dataset):\n",
    "    \"\"\"Dataset that handles corrupt/missing data\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, default_value=0.0, skip_corrupt=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path: Path to CSV file\n",
    "            default_value: Value to use for missing/corrupt data\n",
    "            skip_corrupt: Whether to skip corrupt rows or replace with defaults\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Implement robust CSV loading\n",
    "        # Handle:\n",
    "        # 1. Corrupt rows (wrong data types)\n",
    "        # 2. Missing values (NaN)\n",
    "        # 3. Wrong number of columns\n",
    "        \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Handle corrupt data gracefully\n",
    "        # Options:\n",
    "        # 1. Return default values\n",
    "        # 2. Skip to next valid sample\n",
    "        # 3. Raise specific error types\n",
    "        pass\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Memory Usage Comparison\n",
    "\n",
    "**Task:** Compare memory usage between the two implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "# 1. Test memory inefficient version\n",
    "print(\"Testing Memory Inefficient Dataset:\")\n",
    "# TODO: Instantiate and measure memory\n",
    "\n",
    "# 2. Test your memory efficient version\n",
    "print(\"\\nTesting Your Memory Efficient Dataset:\")\n",
    "# TODO: Instantiate and measure memory\n",
    "\n",
    "# 3. Load a few samples and measure time\n",
    "print(\"\\nTesting sample loading performance:\")\n",
    "# TODO: Time loading of 1000 samples for each\n",
    "\n",
    "# 4. Cleanup\n",
    "os.unlink(temp_csv.name)\n",
    "os.unlink(corrupt_csv.name)\n",
    "print(f\"Cleaned up temporary files\")\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ†• NEW: Part C: Performance Comparison Challenge\n",
    "\n",
    "**Task:** Compare 3 different implementations and analyze trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Implementation1(Dataset):\n",
    "    \"\"\"Implementation 1: Load everything in __init__\"\"\"\n",
    "    def __init__(self, csv_path):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.features = torch.tensor(self.data.iloc[:, :-1].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(self.data.iloc[:, -1].values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx): return self.features[idx], self.labels[idx]\n",
    "\n",
    "class Implementation2(Dataset):\n",
    "    \"\"\"Implementation 2: Load metadata in __init__, data in __getitem__\"\"\"\n",
    "    def __init__(self, csv_path):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.csv_path = csv_path\n",
    "    \n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        return torch.tensor(row.iloc[:-1].values, dtype=torch.float32), \\\n",
    "               torch.tensor(row.iloc[-1], dtype=torch.float32)\n",
    "\n",
    "class Implementation3(Dataset):\n",
    "    \"\"\"Implementation 3: Memory mapping with numpy\"\"\"\n",
    "    def __init__(self, csv_path):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        # Use numpy memmap for large files\n",
    "        self.features = np.array(self.data.iloc[:, :-1].values, dtype=np.float32)\n",
    "        self.labels = np.array(self.data.iloc[:, -1].values, dtype=np.float32)\n",
    "    \n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.features[idx]), torch.from_numpy(self.labels[idx])\n",
    "\n",
    "# =========== PERFORMANCE COMPARISON ===========\n",
    "print(\"Performance Comparison Challenge:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a larger test file\n",
    "large_csv = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
    "large_csv.write(\"feature1,feature2,feature3,feature4,label\\n\")\n",
    "for i in range(10000):  # 10k samples\n",
    "    large_csv.write(f\"{i*1.0},{i*2.0},{i*3.0},{i*4.0},{i%2}\\n\")\n",
    "large_csv.close()\n",
    "\n",
    "print(f\"Large test file created: {large_csv.name}\")\n",
    "\n",
    "# TODO: Implement performance comparison\n",
    "# 1. Measure initialization time for each\n",
    "# 2. Measure memory usage after initialization\n",
    "# 3. Measure time to load 1000 random samples\n",
    "# 4. Create a comparison table\n",
    "# 5. Analyze trade-offs for different scenarios\n",
    "\n",
    "print(\"\\nExpected Output Table:\")\n",
    "print(\"Implementation | Init Time | Memory | Load Time | Best For\")\n",
    "print(\"------------- | --------- | ------ | --------- | --------\")\n",
    "print(\"1 (Eager)     | Fast      | High   | Fast      | Small datasets\")\n",
    "print(\"2 (Lazy)      | Fast      | Low    | Slow      | Huge datasets\")\n",
    "print(\"3 (Memmap)    | Medium    | Medium | Medium    | Medium-large datasets\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(large_csv.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Exercise 2: DataLoader Optimization\n",
    "\n",
    "### Part A: Diagnose and Fix Slow Data Loading\n",
    "\n",
    "**Task:** This training script has slow data loading. Identify the bottlenecks and fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== SLOW TRAINING SCRIPT - FIX ME! ===========\n",
    "class SlowDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000):\n",
    "        self.num_samples = num_samples\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Simulate slow image generation\n",
    "        for i in range(num_samples):\n",
    "            # Simulate image loading/processing\n",
    "            time.sleep(0.001)  # 1ms delay per image\n",
    "            self.images.append(torch.randn(3, 224, 224))\n",
    "            self.labels.append(i % 10)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Slow training setup\n",
    "dataset = SlowDataset(num_samples=100)\n",
    "\n",
    "# PROBLEMATIC DataLoader configuration\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # PROBLEM: No parallel loading\n",
    "    pin_memory=False,  # PROBLEM: Not using pinned memory for GPU\n",
    ")\n",
    "\n",
    "# Simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16, 10)\n",
    ").to(device)\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Starting slow training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(2):\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"Total training time: {time.time() - start_time:.2f} seconds\")\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task:**\n",
    "1. Identify all bottlenecks in the code above\n",
    "2. Rewrite the dataset to be more efficient\n",
    "3. Optimize the DataLoader configuration\n",
    "4. Show performance improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR OPTIMIZED SOLUTION ===========\n",
    "# 1. Create an optimized dataset\n",
    "class OptimizedDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000):\n",
    "        # TODO: Optimize initialization\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Optimize data loading\n",
    "        pass\n",
    "\n",
    "# 2. Create optimized DataLoader\n",
    "# TODO: Choose optimal parameters\n",
    "# dataloader_optimized = DataLoader(...)\n",
    "\n",
    "# 3. Benchmark performance\n",
    "print(\"\\nBenchmarking Optimized Version:\")\n",
    "# TODO: Run training with optimized setup and measure time\n",
    "\n",
    "# 4. Compare performance\n",
    "# TODO: Show speedup factor\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ†• NEW: Part A-2: Handle Corrupt Images Gracefully\n",
    "\n",
    "**Task:** Real-world datasets often have corrupt images. Implement a dataset that handles:\n",
    "1. Corrupt image files\n",
    "2. Missing image files\n",
    "3. Wrong image formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustImageDataset(Dataset):\n",
    "    \"\"\"Dataset that handles corrupt/missing images gracefully\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, label_dict, corrupt_strategy='skip'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir: Directory with images (some may be corrupt)\n",
    "            label_dict: Dict mapping image_name to label\n",
    "            corrupt_strategy: 'skip', 'placeholder', or 'retry'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.label_dict = label_dict\n",
    "        self.corrupt_strategy = corrupt_strategy\n",
    "        \n",
    "        # TODO: Implement\n",
    "        # 1. Scan directory and validate images\n",
    "        # 2. Handle corrupt images based on strategy\n",
    "        # 3. Create list of valid samples\n",
    "        \n",
    "        # Statistics\n",
    "        self.corrupt_count = 0\n",
    "        self.valid_count = 0\n",
    "    \n",
    "    def _validate_image(self, image_path):\n",
    "        \"\"\"Validate if image can be loaded\"\"\"\n",
    "        # TODO: Check if file exists, can be opened, is valid image\n",
    "        pass\n",
    "    \n",
    "    def _get_placeholder(self):\n",
    "        \"\"\"Return placeholder for corrupt images\"\"\"\n",
    "        # TODO: Return gray image or previous valid image\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Implement with error handling\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Profile Data Loading Performance\n",
    "\n",
    "**Task:** Create a profiling tool that measures:\n",
    "1. Batch loading times\n",
    "2. GPU idle time\n",
    "3. Memory usage during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "class DataLoaderProfiler:\n",
    "    \"\"\"Your implementation of a data loading profiler\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'batch_times': [],\n",
    "            'gpu_idle_times': [],\n",
    "            'memory_usage': [],\n",
    "            'cpu_usage': []\n",
    "        }\n",
    "    \n",
    "    def profile_training(self, model, dataloader, num_batches=20):\n",
    "        \"\"\"Profile training loop\"\"\"\n",
    "        # TODO: Implement profiling\n",
    "        pass\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print profiling results\"\"\"\n",
    "        # TODO: Print detailed report\n",
    "        pass\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Visualize metrics\"\"\"\n",
    "        # TODO: Create plots\n",
    "        pass\n",
    "\n",
    "# Test your profiler\n",
    "profiler = DataLoaderProfiler()\n",
    "# TODO: Profile both slow and optimized versions\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ–¼ï¸ Exercise 3: Computer Vision Pipeline\n",
    "\n",
    "### Part A: Create Augmentation Pipeline for Sudanese Agriculture\n",
    "\n",
    "**Task:** Design data augmentations specifically for Sudanese agricultural images.\n",
    "\n",
    "**Considerations:**\n",
    "- Plants might be at different angles\n",
    "- Varying lighting conditions (bright sun vs shade)\n",
    "- Dust/sand particles in air\n",
    "- Different camera angles (from drone vs ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "# 1. Create training augmentations\n",
    "sudanese_agriculture_train_transform = transforms.Compose([\n",
    "    # TODO: Design appropriate augmentations\n",
    "    # Consider: rotation, color jitter, random crop, etc.\n",
    "])\n",
    "\n",
    "# 2. Create validation augmentations (simpler)\n",
    "sudanese_agriculture_val_transform = transforms.Compose([\n",
    "    # TODO: Simple preprocessing for validation\n",
    "])\n",
    "\n",
    "# 3. Create test function\n",
    "def test_augmentations(transform, num_samples=4):\n",
    "    \"\"\"Test and visualize augmentations\"\"\"\n",
    "    # TODO: Create dummy image and apply transformations\n",
    "    # Visualize original + augmented versions\n",
    "    pass\n",
    "\n",
    "# Test your augmentations\n",
    "print(\"Testing Sudanese Agriculture Augmentations:\")\n",
    "test_augmentations(sudanese_agriculture_train_transform)\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ†• NEW: Part A-2: Performance Comparison of Augmentations\n",
    "\n",
    "**Task:** Compare different augmentation strategies for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different augmentation strategies\n",
    "light_aug = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "heavy_aug = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "sudanese_aug = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    # Add dust/sand simulation\n",
    "    transforms.RandomApply([AddDustNoise()], p=0.3),\n",
    "    # Bright sunlight simulation\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness=0.4)], p=0.5),\n",
    "    # Wind effect (blur)\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class AddDustNoise:\n",
    "    \"\"\"Simulate dust/sand particles in air\"\"\"\n",
    "    def __call__(self, img):\n",
    "        # TODO: Implement dust noise\n",
    "        return img\n",
    "\n",
    "# TODO: Benchmark performance\n",
    "# 1. Measure time per batch with each augmentation\n",
    "# 2. Compare GPU utilization\n",
    "# 3. Analyze trade-off between augmentation complexity and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Handle Large Satellite Images\n",
    "\n",
    "**Task:** Create a dataset that can handle very large satellite images (e.g., 10,000Ã—10,000 pixels) without loading them entirely into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "class SatelliteImageDataset(Dataset):\n",
    "    \"\"\"Dataset for large satellite images using tiling\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, tile_size=512, overlap=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths: List of paths to large satellite images\n",
    "            labels: List of labels (e.g., crop type, drought level)\n",
    "            tile_size: Size of tiles to extract\n",
    "            overlap: Overlap between tiles to avoid edge artifacts\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.tile_size = tile_size\n",
    "        self.overlap = overlap\n",
    "        \n",
    "        # TODO: Pre-calculate tile information\n",
    "        # Store tile metadata (image_idx, x, y, label)\n",
    "        self.tiles = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        # TODO: Return number of tiles\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Load only the needed tile from large image\n",
    "        pass\n",
    "    \n",
    "    def visualize_tile(self, idx, show_grid=True):\n",
    "        \"\"\"Visualize a tile within the context of the full image\"\"\"\n",
    "        # TODO: Implement visualization\n",
    "        pass\n",
    "\n",
    "# Test with simulated large images\n",
    "print(\"Creating test satellite images...\")\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "\n",
    "# TODO: Create test images and test your dataset\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree(temp_dir)\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ†• NEW: Part B-2: Performance Optimization for Satellite Images\n",
    "\n",
    "**Task:** Optimize satellite image loading for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_satellite_strategies():\n",
    "    \"\"\"Compare different strategies for handling large satellite images\"\"\"\n",
    "    \n",
    "    strategies = {\n",
    "        'naive': 'Load entire image, then crop',\n",
    "        'tiling': 'Pre-compute tiles',\n",
    "        'streaming': 'Stream tiles on demand',\n",
    "        'memmap': 'Memory map the image file',\n",
    "    }\n",
    "    \n",
    "    # TODO: Implement benchmark\n",
    "    # 1. Create large test image (5000x5000)\n",
    "    # 2. Measure time to load 100 random tiles with each strategy\n",
    "    # 3. Measure memory usage\n",
    "    # 4. Create comparison table\n",
    "    \n",
    "    print(\"Strategy Comparison:\")\n",
    "    print(\"Strategy   | Load Time | Memory | Best For\")\n",
    "    print(\"---------- | --------- | ------ | --------\")\n",
    "    print(\"Naive      | Slow      | High   | Small images\")\n",
    "    print(\"Tiling     | Medium    | Medium | Medium images, random access\")\n",
    "    print(\"Streaming  | Fast      | Low    | Sequential access\")\n",
    "    print(\"Memmap     | Fast      | Low    | Random access, large images\")\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_satellite_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 4: NLP Pipeline for Arabic Text\n",
    "\n",
    "### Part A: Handle Sudanese Arabic Dialect\n",
    "\n",
    "**Task:** Create a text dataset that handles Sudanese Arabic dialect features:\n",
    "1. Right-to-left text\n",
    "2. Dialect-specific words\n",
    "3. Handle both Modern Standard Arabic and Sudanese dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "class SudaneseArabicDataset(Dataset):\n",
    "    \"\"\"Dataset for Sudanese Arabic text classification\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, vocab=None, max_length=128, \n",
    "                 handle_dialect=True, normalize=True):\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "        self.handle_dialect = handle_dialect\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # TODO: Build vocabulary considering dialect\n",
    "        if vocab is None:\n",
    "            self.vocab = self._build_vocab(texts)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        \n",
    "        # TODO: Add special tokens\n",
    "        \n",
    "    def _build_vocab(self, texts):\n",
    "        \"\"\"Build vocabulary with dialect handling\"\"\"\n",
    "        # TODO: Implement vocabulary building\n",
    "        # Consider: dialect normalization, MSA mapping, etc.\n",
    "        pass\n",
    "    \n",
    "    def _preprocess_text(self, text):\n",
    "        \"\"\"Preprocess Arabic text\"\"\"\n",
    "        # TODO: Implement preprocessing steps:\n",
    "        # 1. Normalize Arabic characters\n",
    "        # 2. Remove diacritics (optional)\n",
    "        # 3. Handle dialect words (map to MSA or keep)\n",
    "        # 4. Other cleaning steps\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Implement text encoding\n",
    "        pass\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Convert token IDs back to text\"\"\"\n",
    "        # TODO: Implement decoding\n",
    "        pass\n",
    "\n",
    "# Test data\n",
    "sudanese_texts = [\n",
    "    \"ÙƒÙØ³Ø±Ø© Ø¨ØªØ§Ø¹Ø© ÙÙˆÙ„ Ù…Ø¹ Ø·Ù…Ø§Ø·Ù…\",  # Sudanese dialect\n",
    "    \"Ø§Ù„Ø·Ù‚Ø³ Ø§Ù„ÙŠÙˆÙ… Ø­Ø§Ø± Ø¬Ø¯Ø§Ù‹\",  # Modern Standard Arabic\n",
    "    \"Ø´Ø§ÙŠÙ Ø§Ù„Ù‚ÙˆÙ… Ø¯Ù‡ Ø¹Ø§Ù…Ù„ÙŠÙ† Ø¥Ø²Ø§ÙŠ\",  # Sudanese dialect\n",
    "    \"Ø§Ù„Ø²Ø±Ø§Ø¹Ø© ÙÙŠ Ø§Ù„Ø³ÙˆØ¯Ø§Ù† Ù…ØªÙ‚Ø¯Ù…Ø©\",  # MSA\n",
    "    \"Ø¹Ø§ÙŠØ² Ø£Ø´ÙˆÙ ÙƒÙ…Ø§Ø´Ø©\",  # Sudanese dialect\n",
    "]\n",
    "\n",
    "labels = [0, 1, 0, 1, 0]  # 0 = dialect, 1 = MSA\n",
    "\n",
    "# Test your dataset\n",
    "print(\"Testing Sudanese Arabic Dataset:\")\n",
    "dataset = SudaneseArabicDataset(sudanese_texts, labels)\n",
    "# TODO: Test encoding/decoding\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ†• NEW: Part A-2: Handle Noisy/Corrupt Arabic Text\n",
    "\n",
    "**Task:** Real-world Arabic text from social media/SMS is often noisy. Handle:\n",
    "1. Mixed Arabic/English/Latin script\n",
    "2. Missing diacritics\n",
    "3. Spelling variations\n",
    "4. Emojis and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyArabicDataset(Dataset):\n",
    "    \"\"\"Dataset that handles noisy Arabic text\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, cleaning_strategy='aggressive'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: List of noisy Arabic texts\n",
    "            labels: Corresponding labels\n",
    "            cleaning_strategy: 'light', 'aggressive', or 'smart'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.strategy = cleaning_strategy\n",
    "        \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Clean noisy Arabic text\"\"\"\n",
    "        # TODO: Implement cleaning strategies\n",
    "        # Light: Remove emojis, extra spaces\n",
    "        # Aggressive: Normalize all variations, remove Latin\n",
    "        # Smart: Try to preserve meaning while cleaning\n",
    "        pass\n",
    "    \n",
    "    def _normalize_arabic(self, text):\n",
    "        \"\"\"Normalize Arabic characters\"\"\"\n",
    "        # TODO: Normalize different forms of same character\n",
    "        pass\n",
    "    \n",
    "    def _handle_mixed_script(self, text):\n",
    "        \"\"\"Handle text with mixed Arabic/Latin script\"\"\"\n",
    "        # TODO: Convert numbers, transliterations, etc.\n",
    "        pass\n",
    "\n",
    "# Test with noisy data\n",
    "noisy_texts = [\n",
    "    \"Ø§Ù„Ø·Ù‚Ø³ Ø­Ø§Ø± Ø¬Ø¯Ø§ â˜€ï¸ðŸ”¥\",  # With emojis\n",
    "    \"Ø§Ù„Ø³Ø¹Ø± 150 Ø¬Ù†ÙŠÛ\",  # Mixed Arabic/English numbers\n",
    "    \"Ù…Ø´ØºÙˆÙ„ Ø­Ø§Ù„ÙŠØ§...\",  # With punctuation\n",
    "    \"I love Ø§Ù„Ø³ÙˆØ¯Ø§Ù† â¤ï¸\",  # Mixed languages\n",
    "    \"Ø£Ù†Ø§ Ø¬Ø¹Ø§Ù† ðŸ”\",  # With emoji\n",
    "]\n",
    "\n",
    "# TODO: Test cleaning strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Streaming Dataset for Large Text Corpora\n",
    "\n",
    "**Task:** Create a streaming dataset that can handle text files larger than memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "class StreamingArabicNews(IterableDataset):\n",
    "    \"\"\"Streaming dataset for Arabic news articles\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, vocab=None, max_length=128, \n",
    "                 buffer_size=1000, shuffle=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_path: Path to large text file (one article per line)\n",
    "            vocab: Pre-built vocabulary\n",
    "            max_length: Maximum sequence length\n",
    "            buffer_size: Number of lines to buffer\n",
    "            shuffle: Whether to shuffle the stream\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.vocab = vocab or self._build_vocab_from_file()\n",
    "        self.max_length = max_length\n",
    "        self.buffer_size = buffer_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # TODO: Initialize\n",
    "    \n",
    "    def _build_vocab_from_file(self):\n",
    "        \"\"\"Build vocabulary by streaming through file once\"\"\"\n",
    "        # TODO: Implement vocabulary building from stream\n",
    "        pass\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Stream data from file\"\"\"\n",
    "        # TODO: Implement streaming logic\n",
    "        # Consider: worker splitting, buffering, shuffling\n",
    "        pass\n",
    "\n",
    "# Create test large text file\n",
    "print(\"Creating test text file...\")\n",
    "temp_text_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8')\n",
    "\n",
    "# Generate Arabic text\n",
    "arabic_samples = [\n",
    "    \"ØªÙ‚Ø±ÙŠØ± Ø¹Ù† Ø§Ù„Ø²Ø±Ø§Ø¹Ø© ÙÙŠ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†\",\n",
    "    \"Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø±ÙŠØ§Ø¶Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©\",\n",
    "    \"ØªØ·ÙˆØ±Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ù…Ø§Ù„ÙŠØ©\",\n",
    "    \"Ø§Ù„Ø·Ù‚Ø³ ÙˆØ£Ø­ÙˆØ§Ù„ Ø§Ù„Ø²Ø±Ø§Ø¹Ø©\",\n",
    "    \"Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø±ÙŠÙÙŠØ©\",\n",
    "]\n",
    "\n",
    "# Write many lines to simulate large file\n",
    "for i in range(100):\n",
    "    for sample in arabic_samples:\n",
    "        temp_text_file.write(f\"{sample} - Ø§Ù„Ù†Ø³Ø®Ø© {i}\\n\")\n",
    "temp_text_file.close()\n",
    "\n",
    "print(f\"Test file created: {temp_text_file.name} ({os.path.getsize(temp_text_file.name)} bytes)\")\n",
    "\n",
    "# Test your streaming dataset\n",
    "print(\"\\nTesting Streaming Dataset:\")\n",
    "# TODO: Test the streaming dataset\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(temp_text_file.name)\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ†• NEW: Part B-2: Performance Comparison for Text Datasets\n",
    "\n",
    "**Task:** Compare different text processing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_text_processing():\n",
    "    \"\"\"Compare text processing strategies\"\"\"\n",
    "    \n",
    "    strategies = {\n",
    "        'simple': 'Split on whitespace',\n",
    "        'arabic_tokenizer': 'Arabic-specific tokenizer',\n",
    "        'transformers': 'HuggingFace tokenizer',\n",
    "        'character_level': 'Character-level encoding',\n",
    "    }\n",
    "    \n",
    "    # TODO: Implement benchmark\n",
    "    # 1. Process 10,000 Arabic sentences with each strategy\n",
    " # 2. Measure processing time\n",
    "    # 3. Measure memory usage\n",
    "    # 4. Compare vocabulary sizes\n",
    "    \n",
    "    print(\"Text Processing Strategy Comparison:\")\n",
    "    print(\"Strategy         | Speed   | Memory | Vocab Size | Accuracy\")\n",
    "    print(\"---------------- | ------- | ------ | ---------- | --------\")\n",
    "    print(\"Simple split     | Fast    | Low    | Large      | Low\")\n",
    "    print(\"Arabic tokenizer | Medium  | Medium | Medium     | High\")\n",
    "    print(\"Transformers     | Slow    | High   | Large      | Highest\")\n",
    "    print(\"Character level  | Fast    | Low    | Small      | Medium\")\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_text_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Challenge Problems\n",
    "\n",
    "### ðŸ†• NEW: Challenge 0: Debugging Real-World Sudanese Data Pipeline\n",
    "\n",
    "**Task:** Debug this real-world Sudanese data pipeline with multiple issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== BUGGY SUDANESE PIPELINE - DEBUG ME! ===========\n",
    "class BuggySudanesePipeline:\n",
    "    \"\"\"Buggy pipeline for Sudanese agricultural data\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # Load all data at once (problem for large datasets)\n",
    "        self.images = []\n",
    "        self.prices = []\n",
    "        self.dates = []\n",
    "        \n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith('.jpg'):\n",
    "                # Load image\n",
    "                img = Image.open(os.path.join(data_dir, file))\n",
    "                self.images.append(img)  # PROBLEM: Storing PIL images\n",
    "                \n",
    "                # Parse metadata from filename\n",
    "                parts = file.split('_')\n",
    "                self.prices.append(float(parts[1]))  # No error handling\n",
    "                self.dates.append(parts[2])  # Assuming format is correct\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        \"\"\"Create PyTorch dataset\"\"\"\n",
    "        class SudaneseDataset(Dataset):\n",
    "            def __init__(self, images, prices, dates):\n",
    "                self.images = images  # PROBLEM: Passing PIL images\n",
    "                self.prices = prices\n",
    "                self.dates = dates\n",
    "                \n",
    "                # Random transform each time\n",
    "                self.transform = transforms.RandomRotation(30)\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.images)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                # PROBLEM: No tensor conversion\n",
    "                img = self.images[idx]\n",
    "                \n",
    "                # PROBLEM: Different augmentation each call\n",
    "                if random.random() > 0.5:\n",
    "                    img = self.transform(img)\n",
    "                \n",
    "                # Convert price to tensor (but price might be missing)\n",
    "                price = torch.tensor(self.prices[idx])\n",
    "                \n",
    "                return img, price\n",
    "        \n",
    "        return SudaneseDataset(self.images, self.prices, self.dates)\n",
    "    \n",
    "    def create_dataloader(self, batch_size=32):\n",
    "        \"\"\"Create DataLoader\"\"\"\n",
    "        dataset = self.create_dataset()\n",
    "        \n",
    "        # PROBLEM: Inefficient DataLoader settings\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # No parallel loading\n",
    "            collate_fn=self.buggy_collate  # Custom buggy collate\n",
    "        )\n",
    "    \n",
    "    def buggy_collate(self, batch):\n",
    "        \"\"\"Buggy collate function\"\"\"\n",
    "        images, prices = zip(*batch)\n",
    "        \n",
    "        # PROBLEM: Assuming all images same size\n",
    "        images = torch.stack(images)\n",
    "        prices = torch.stack(prices)\n",
    "        \n",
    "        return images, prices\n",
    "\n",
    "# TODO: Identify and fix ALL bugs in this pipeline\n",
    "# List at least 10 different bugs and their fixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Multi-Modal Dataset (Images + Text)\n",
    "\n",
    "**Task:** Create a dataset that handles both images and text for a Sudanese market monitoring system.\n",
    "\n",
    "**Scenario:** You're building a system that:\n",
    "- Takes photos of market goods (sorghum, millet, wheat)\n",
    "- Has Arabic text descriptions from sellers\n",
    "- Includes price information\n",
    "- Needs to predict whether prices are reasonable\n",
    "\n",
    "**Requirements:**\n",
    "1. Handle image loading and augmentation\n",
    "2. Process Arabic text descriptions\n",
    "3. Combine multiple data types in single sample\n",
    "4. Handle missing data (some samples might have only image or only text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== CHALLENGE 1 ===========\n",
    "class SudaneseMarketMultiModalDataset(Dataset):\n",
    "    \"\"\"Multi-modal dataset for Sudanese market monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 image_dir,  # Directory with images\n",
    "                 metadata_file,  # CSV with text, prices, etc.\n",
    "                 image_transform=None,\n",
    "                 text_max_length=50,\n",
    "                 handle_missing='zero'):  # How to handle missing data\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir: Directory containing product images\n",
    "            metadata_file: CSV with columns: image_name, description, price, category, is_reasonable\n",
    "            image_transform: Transformations for images\n",
    "            text_max_length: Maximum text sequence length\n",
    "            handle_missing: Strategy for missing data ('zero', 'mean', 'ignore')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_dir = Path(image_dir)\n",
    "        \n",
    "        # TODO: Load metadata\n",
    "        self.metadata = pd.read_csv(metadata_file)\n",
    "        \n",
    "        # TODO: Initialize image transformations\n",
    "        self.image_transform = image_transform or self._default_image_transform()\n",
    "        \n",
    "        # TODO: Initialize text processing\n",
    "        self.text_max_length = text_max_length\n",
    "        self.text_vocab = self._build_text_vocab()\n",
    "        \n",
    "        # TODO: Handle missing data strategy\n",
    "        self.handle_missing = handle_missing\n",
    "        \n",
    "        # TODO: Preprocess data\n",
    "        \n",
    "    def _default_image_transform(self):\n",
    "        \"\"\"Default image transformations for market products\"\"\"\n",
    "        # TODO: Design appropriate transformations\n",
    "        pass\n",
    "    \n",
    "    def _build_text_vocab(self):\n",
    "        \"\"\"Build vocabulary from Arabic descriptions\"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return multi-modal sample\"\"\"\n",
    "        row = self.metadata.iloc[idx]\n",
    "        \n",
    "        # TODO: Load and process image\n",
    "        image_tensor = None  # Load image if exists\n",
    "        \n",
    "        # TODO: Process Arabic text\n",
    "        text_tensor = None  # Encode text if exists\n",
    "        \n",
    "        # TODO: Handle price/numerical features\n",
    "        price_tensor = None\n",
    "        \n",
    "        # TODO: Handle missing data\n",
    "        \n",
    "        # TODO: Combine into single sample\n",
    "        # Sample structure:\n",
    "        # {\n",
    "        #     'image': image_tensor,\n",
    "        #     'text': text_tensor,\n",
    "        #     'price': price_tensor,\n",
    "        #     'label': row['is_reasonable']  # 0 or 1\n",
    "        # }\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Custom collate function for multi-modal data\"\"\"\n",
    "        # TODO: Implement collate function that handles:\n",
    "        # - Variable length sequences\n",
    "        # - Missing modalities\n",
    "        # - Different data types\n",
    "        pass\n",
    "\n",
    "# Create test data\n",
    "print(\"Creating test multi-modal data...\")\n",
    "temp_mm_dir = Path(tempfile.mkdtemp())\n",
    "\n",
    "# TODO: Create test images and metadata\n",
    "# 1. Create image directory with dummy images\n",
    "# 2. Create CSV metadata file\n",
    "\n",
    "# Test your dataset\n",
    "print(\"\\nTesting Multi-Modal Dataset:\")\n",
    "# dataset = SudaneseMarketMultiModalDataset(...)\n",
    "# dataloader = DataLoader(dataset, batch_size=4, collate_fn=dataset.collate_fn)\n",
    "# TODO: Test batch loading\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree(temp_mm_dir)\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Data Pipeline for Sudanese Healthcare\n",
    "\n",
    "**Task:** Design a complete data pipeline for medical imaging in Sudanese hospitals.\n",
    "\n",
    "**Special Considerations:**\n",
    "1. Handle DICOM files (medical images)\n",
    "2. Include patient metadata\n",
    "3. Respect patient privacy (anonymization)\n",
    "4. Work with limited internet connectivity (offline capable)\n",
    "5. Handle power outages (checkpointing)\n",
    "\n",
    "**Bonus:** Implement data validation to catch corrupted files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== CHALLENGE 2 ===========\n",
    "class SudaneseHealthcareDataset(Dataset):\n",
    "    \"\"\"Dataset for Sudanese healthcare applications\"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, transform=None, anonymize=True,\n",
    "                 validate_data=True, cache_size=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_root: Root directory with structure:\n",
    "                - images/ (DICOM or PNG files)\n",
    "                - metadata.csv (patient info, diagnoses)\n",
    "                - annotations/ (optional: segmentation masks)\n",
    "            transform: Image transformations\n",
    "            anonymize: Whether to anonymize patient data\n",
    "            validate_data: Validate file integrity\n",
    "            cache_size: Number of samples to cache in memory\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_root = Path(data_root)\n",
    "        \n",
    "        # TODO: Implement with healthcare-specific considerations\n",
    "        \n",
    "    def _load_and_validate_dicom(self, filepath):\n",
    "        \"\"\"Load and validate DICOM file\"\"\"\n",
    "        # TODO: Implement DICOM loading with validation\n",
    "        pass\n",
    "    \n",
    "    def _anonymize_metadata(self, metadata):\n",
    "        \"\"\"Remove personally identifiable information\"\"\"\n",
    "        # TODO: Implement anonymization\n",
    "        pass\n",
    "    \n",
    "    def _checkpoint_state(self):\n",
    "        \"\"\"Save dataset state for recovery from power outages\"\"\"\n",
    "        # TODO: Implement checkpointing\n",
    "        pass\n",
    "    \n",
    "    def _restore_from_checkpoint(self):\n",
    "        \"\"\"Restore dataset state\"\"\"\n",
    "        # TODO: Implement restoration\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "# Create a DataValidator class\n",
    "class HealthcareDataValidator:\n",
    "    \"\"\"Validate healthcare data integrity\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_dicom(filepath):\n",
    "        \"\"\"Validate DICOM file integrity\"\"\"\n",
    "        # TODO: Check if DICOM is valid and not corrupted\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_metadata(metadata):\n",
    "        \"\"\"Validate patient metadata\"\"\"\n",
    "        # TODO: Check required fields, data types, ranges\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_anonymization(metadata):\n",
    "        \"\"\"Check if data is properly anonymized\"\"\"\n",
    "        # TODO: Verify no PII remains\n",
    "        pass\n",
    "\n",
    "# Design document\n",
    "print(\"\"\"\n",
    "Design Considerations for Sudanese Healthcare Pipeline:\n",
    "\n",
    "1. OFFLINE OPERATION:\n",
    "   - Local caching of all data\n",
    "   - Pre-processed datasets stored locally\n",
    "   - Batch processing for when connectivity is available\n",
    "\n",
    "2. POWER RESILIENCE:\n",
    "   - Regular checkpointing of dataset state\n",
    "   - Incremental processing with recovery\n",
    "   - Battery backup considerations\n",
    "\n",
    "3. PRIVACY:\n",
    "   - Automatic anonymization of patient data\n",
    "   - Encryption of sensitive data\n",
    "   - Access controls\n",
    "\n",
    "4. VALIDATION:\n",
    "   - File integrity checks\n",
    "   - Data completeness validation\n",
    "   - Cross-field consistency checks\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Implement test cases for your design\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ†• NEW: Performance Comparison Final Challenge\n",
    "\n",
    "**Task:** Create a comprehensive performance comparison of different data pipeline strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_performance_benchmark():\n",
    "    \"\"\"Comprehensive benchmark of different data pipeline strategies\"\"\"\n",
    "    \n",
    "    print(\"Comprehensive Performance Benchmark\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    scenarios = [\n",
    "        (\"Small dataset (1GB)\", 1000, \"all_in_memory\"),\n",
    "        (\"Medium dataset (10GB)\", 10000, \"lazy_loading\"),\n",
    "        (\"Large dataset (100GB)\", 100000, \"streaming\"),\n",
    "        (\"Mixed modalities\", 5000, \"multi_modal\"),\n",
    "        (\"Corrupt data (10% corrupt)\", 1000, \"robust_loading\"),\n",
    "    ]\n",
    "    \n",
    "    strategies = {\n",
    "        \"naive\": \"Basic implementation (num_workers=0, no caching)\",\n",
    "        \"optimized\": \"Optimized (num_workers=4, pin_memory=True, prefetch)\",\n",
    "        \"memory_mapped\": \"Memory mapping for large files\",\n",
    "        \"streaming\": \"IterableDataset for streaming\",\n",
    "        \"cached\": \"CachedDataset with LRU cache\",\n",
    "    }\n",
    "    \n",
    "    # TODO: Implement comprehensive benchmark\n",
    "    # For each scenario and strategy:\n",
    "    # 1. Measure initialization time\n",
    "    # 2. Measure memory usage\n",
    "    # 3. Measure time to load 100 batches\n",
    "    # 4. Measure GPU utilization\n",
    "    # 5. Create comparison table\n",
    "    \n",
    "    print(\"\\nExpected Results Summary:\")\n",
    "    print(\"Scenario               | Best Strategy      | Why\")\n",
    "    print(\"---------------------- | ------------------ | ---\")\n",
    "    print(\"Small dataset          | naive/optimized    | Overhead not worth it\")\n",
    "    print(\"Medium dataset         | optimized          | Balanced perf/memory\")\n",
    "    print(\"Large dataset          | streaming          | Memory constraints\")\n",
    "print(\"Mixed modalities      | cached            | Repeated access\")\n",
    "    print(\"Corrupt data          | robust_loading     | Error handling needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Assessment Questions\n",
    "\n",
    "Answer these questions in markdown cells:\n",
    "\n",
    "### Q1: When should you use `num_workers=0` in DataLoader? What are the trade-offs?\n",
    "\n",
    "### Q2: What's the difference between `pin_memory=True` and `pin_memory=False`? When would you use each?\n",
    "\n",
    "### Q3: How does `prefetch_factor` affect performance and memory usage?\n",
    "\n",
    "### Q4: What are the main differences between `Dataset` and `IterableDataset`? Give examples of when to use each.\n",
    "\n",
    "### Q5: How would you handle a dataset where some samples have corrupted files?\n",
    "\n",
    "### Q6: What special considerations are needed for Arabic text processing vs English?\n",
    "\n",
    "### Q7: How would you design a data pipeline that works in areas with intermittent internet connectivity?\n",
    "\n",
    "### ðŸ†• Q8: Compare 3 different strategies for handling large images (naive loading, tiling, memory mapping). When would you use each?\n",
    "\n",
    "### ðŸ†• Q9: How would you debug a data pipeline that's slower than expected? List the steps you would take.\n",
    "\n",
    "### ðŸ†• Q10: Design a data validation pipeline for Sudanese agricultural images. What checks would you implement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Progress Tracker\n",
    "\n",
    "Check off exercises as you complete them:\n",
    "\n",
    "- [ ] **Debugging Exercise 0**: Find and fix bugs in buggy dataset\n",
    "- [ ] **Exercise 1A**: Fix Memory-Inefficient Dataset\n",
    "- [ ] **Exercise 1A-2**: Handle corrupt/missing data\n",
    "- [ ] **Exercise 1B**: Memory Usage Comparison\n",
    "- [ ] **Exercise 1C**: Performance Comparison Challenge\n",
    "- [ ] **Exercise 2A**: Diagnose & Fix Slow Data Loading\n",
    "- [ ] **Exercise 2A-2**: Handle corrupt images gracefully\n",
    "- [ ] **Exercise 2B**: Profile Data Loading Performance\n",
    "- [ ] **Exercise 3A**: Sudanese Agriculture Augmentations\n",
    "- [ ] **Exercise 3A-2**: Performance comparison of augmentations\n",
    "- [ ] **Exercise 3B**: Large Satellite Images Dataset\n",
    "- [ ] **Exercise 3B-2**: Performance optimization for satellite images\n",
    "- [ ] **Exercise 4A**: Sudanese Arabic Dialect Dataset\n",
    "- [ ] **Exercise 4A-2**: Handle noisy/corrupt Arabic text\n",
    "- [ ] **Exercise 4B**: Streaming Text Dataset\n",
    "- [ ] **Exercise 4B-2**: Performance comparison for text datasets\n",
    "- [ ] **Challenge 0**: Debugging Real-World Sudanese Data Pipeline\n",
    "- [ ] **Challenge 1**: Multi-Modal Dataset (Images + Text)\n",
    "- [ ] **Challenge 2**: Sudanese Healthcare Pipeline\n",
    "- [ ] **Final Challenge**: Comprehensive Performance Benchmark\n",
    "- [ ] **Assessment Questions Q1-Q10**\n",
    "\n",
    "## ðŸ† Completion Certificate\n",
    "\n",
    "Once you complete all exercises, you've mastered:\n",
    "- âœ… PyTorch Dataset design patterns\n",
    "- âœ… DataLoader optimization techniques\n",
    "- âœ… Computer vision pipelines with augmentation\n",
    "- âœ… NLP pipelines for Arabic text\n",
    "- âœ… Multi-modal data handling\n",
    "- âœ… Production considerations for Sudanese context\n",
    "- âœ… ðŸ†• Debugging and performance optimization skills\n",
    "- âœ… ðŸ†• Handling corrupt/missing data\n",
    "- âœ… ðŸ†• Performance comparison and analysis\n",
    "\n",
    "**You're ready for Lecture 3: Advanced Model Architectures & Training!** ðŸŽ‰\n",
    "\n",
    "## ðŸ’¡ Tips for Success\n",
    "\n",
    "1. **Start Simple**: Begin with basic implementations, then optimize\n",
    "2. **Profile Early**: Use the profiler to identify bottlenecks\n",
    "3. **Test with Small Data**: Verify correctness before scaling up\n",
    "4. **Consider Sudanese Context**: Think about real-world constraints\n",
    "5. **Document Your Choices**: Explain why you made certain design decisions\n",
    "6. **ðŸ†• Test Edge Cases**: Always test with corrupt/missing data\n",
    "7. **ðŸ†• Compare Performance**: Benchmark different approaches\n",
    "8. **ðŸ†• Debug Systematically**: Learn to identify and fix bugs efficiently\n",
    "\n",
    "## ðŸ¤ Need Help?\n",
    "\n",
    "- Review Lecture 2 notebook for concepts\n",
    "- Use PyTorch documentation for specific APIs\n",
    "- Test your implementations step by step\n",
    "- Consider edge cases (missing data, large files, etc.)\n",
    "- ðŸ†• Use debugging tools: pdb, print statements, profiling\n",
    "- ðŸ†• Create minimal reproducible examples when debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Important Note:\n",
    "# Go to Chapter 10 of Hands On Machine Learning with sklearn and PyTorch by AurÃ©lien GÃ©ron.and solve the exercises at the end of the chapter.and add it in this notebook as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
