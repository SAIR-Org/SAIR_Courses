{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö PyTorch Practice Notebook - Lecture 1 Exercises\n",
    "\n",
    "**Based on:** SAIR PyTorch Mastery - Lecture 1: From NumPy to Production Neural Networks\n",
    "\n",
    "**Instructions:** Complete the exercises below to test your understanding of PyTorch fundamentals. Try to solve them without looking at the original notebook first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup & Imports\n",
    "\n",
    "Run this cell first to set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 1: Understanding Tensors & Autograd\n",
    "\n",
    "### Part A: Manual Gradient Verification\n",
    "\n",
    "**Task:** Create a simple computation graph and compute gradients both manually and with PyTorch's autograd.\n",
    "\n",
    "Given:\n",
    "- x = 3.0\n",
    "- w = 2.0  \n",
    "- b = 1.0\n",
    "\n",
    "Compute: y = w*x + b, then loss = y¬≤\n",
    "\n",
    "**Your job:**\n",
    "1. Create PyTorch tensors with gradient tracking\n",
    "2. Compute the forward pass\n",
    "3. Compute gradients using `.backward()`\n",
    "4. Verify by manually computing ‚àÇloss/‚àÇw and ‚àÇloss/‚àÇx using chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "# Create tensors with requires_grad=True\n",
    "\n",
    "# Forward pass: y = w*x + b, loss = y¬≤\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "# Print gradients\n",
    "\n",
    "# Manual verification\n",
    "# Compute manually: ‚àÇloss/‚àÇw = ? and ‚àÇloss/‚àÇx = ?\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Gradient Accumulation Demonstration\n",
    "\n",
    "**Task:** Show what happens when you forget `optimizer.zero_grad()`.\n",
    "\n",
    "1. Create a simple linear model\n",
    "2. Run two training steps WITHOUT zero_grad\n",
    "3. Show that gradients accumulate\n",
    "4. Fix by adding zero_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "# Create a simple model\n",
    "# model = nn.Linear(3, 1)\n",
    "\n",
    "# Create dummy data\n",
    "# X = torch.randn(5, 3)\n",
    "# y = torch.randn(5, 1)\n",
    "\n",
    "# Create optimizer\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# First forward-backward (no zero_grad)\n",
    "# Print gradient after first iteration\n",
    "\n",
    "# Second forward-backward (no zero_grad)\n",
    "# Print gradient after second iteration\n",
    "\n",
    "# Show that gradients doubled!\n",
    "\n",
    "# Now do it correctly with zero_grad\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Exercise 2: Building Neural Networks with nn.Module\n",
    "\n",
    "### Part A: Convert NumPy Network to PyTorch\n",
    "\n",
    "**Task:** Convert this NumPy-style network to PyTorch using `nn.Module`.\n",
    "\n",
    "Original NumPy network:\n",
    "```python\n",
    "class NumPyNetwork:\n",
    "    def __init__(self):\n",
    "        self.W1 = np.random.randn(10, 20)\n",
    "        self.b1 = np.zeros(20)\n",
    "        self.W2 = np.random.randn(20, 5)\n",
    "        self.b2 = np.zeros(5)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        z1 = X @ self.W1 + self.b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        return z2\n",
    "```\n",
    "\n",
    "Create a PyTorch version with:\n",
    "1. Proper inheritance from `nn.Module`\n",
    "2. PyTorch layers instead of manual weights\n",
    "3. ReLU activation\n",
    "4. Forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "class PyTorchNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Implement forward pass\n",
    "        pass\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Test Your Network\n",
    "\n",
    "Test that your network works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your network\n",
    "model = PyTorchNetwork()\n",
    "\n",
    "# Create dummy input\n",
    "X_test = torch.randn(8, 10)  # batch_size=8, features=10\n",
    "\n",
    "# Forward pass\n",
    "output = model(X_test)\n",
    "\n",
    "print(f\"Input shape: {X_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Exercise 3: Complete Training Loop\n",
    "\n",
    "### Part A: Fix the Buggy Training Loop\n",
    "\n",
    "**Task:** This training loop has several bugs. Identify and fix them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== BUGGY CODE - FIX ME! ===========\n",
    "class BuggyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(5, 10)\n",
    "        self.layer2 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        return self.layer2(x)\n",
    "\n",
    "# Create model and optimizer\n",
    "model = BuggyNet()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Dummy data\n",
    "X = torch.randn(100, 5)\n",
    "y = torch.randn(100, 1)\n",
    "\n",
    "# Training loop with bugs\n",
    "losses = []\n",
    "for epoch in range(50):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = ((predictions - y) ** 2).mean()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (Should Decrease!)')\n",
    "plt.show()\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Write a Correct Training Loop\n",
    "\n",
    "**Task:** Write a complete, correct training loop from scratch for a regression problem.\n",
    "\n",
    "Requirements:\n",
    "1. Create a neural network with 2 hidden layers\n",
    "2. Use proper loss function for regression\n",
    "3. Include all 6 training steps\n",
    "4. Track and plot loss\n",
    "5. Make it device-agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "# 1. Define your network architecture\n",
    "\n",
    "# 2. Set device (CPU or GPU)\n",
    "\n",
    "# 3. Create dummy data\n",
    "# X = torch.randn(200, 7)  # 200 samples, 7 features\n",
    "# y = torch.randn(200, 1)  # 200 target values\n",
    "\n",
    "# 4. Initialize model, loss function, optimizer\n",
    "\n",
    "# 5. Training loop (100 epochs)\n",
    "# losses = []\n",
    "# for epoch in range(100):\n",
    "#     # Training steps here...\n",
    "#     pass\n",
    "\n",
    "# 6. Plot loss curve\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Exercise 4: Model Persistence\n",
    "\n",
    "### Part A: Save and Load Model Weights\n",
    "\n",
    "**Task:** Train a simple model, save it, load it into a new model, and verify they produce identical predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "# 1. Create and train a simple model\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(4, 10),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(10, 1)\n",
    "# )\n",
    "\n",
    "# 2. Train for a few epochs\n",
    "\n",
    "# 3. Save model weights\n",
    "\n",
    "# 4. Create a NEW model with same architecture\n",
    "# new_model = ...\n",
    "\n",
    "# 5. Load saved weights into new model\n",
    "\n",
    "# 6. Verify predictions match\n",
    "# test_input = torch.randn(5, 4)\n",
    "# original_pred = model(test_input)\n",
    "# loaded_pred = new_model(test_input)\n",
    "# print(f\"Predictions match: {torch.allclose(original_pred, loaded_pred, rtol=1e-4)}\")\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Create Training Checkpoints\n",
    "\n",
    "**Task:** Save a checkpoint that allows resuming training. Include:\n",
    "- Model state_dict\n",
    "- Optimizer state_dict\n",
    "- Epoch number\n",
    "- Loss value\n",
    "\n",
    "Then demonstrate loading and resuming training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== YOUR CODE HERE ===========\n",
    "# 1. Train a model for 30 epochs\n",
    "\n",
    "# 2. Create checkpoint dictionary\n",
    "# checkpoint = {\n",
    "#     'epoch': 30,\n",
    "#     'model_state_dict': ...,\n",
    "#     'optimizer_state_dict': ...,\n",
    "#     'loss': ...,\n",
    "# }\n",
    "\n",
    "# 3. Save checkpoint\n",
    "\n",
    "# 4. Create new model and optimizer\n",
    "\n",
    "# 5. Load checkpoint\n",
    "\n",
    "# 6. Resume training from epoch 30\n",
    "# for epoch in range(checkpoint['epoch'], 60):\n",
    "#     # Training loop\n",
    "#     print(f\"Resumed training at epoch {epoch}\")\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Exercise 5: Real-World Application\n",
    "\n",
    "### Part A: Boston Housing Prediction\n",
    "\n",
    "**Task:** Use PyTorch to predict Boston housing prices.\n",
    "\n",
    "Steps:\n",
    "1. Load Boston housing dataset\n",
    "2. Split into train/test\n",
    "3. Standardize features\n",
    "4. Create PyTorch model\n",
    "5. Train model\n",
    "6. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =========== YOUR CODE HERE ===========\n",
    "# 1. Load dataset\n",
    "# boston = load_boston()\n",
    "# X, y = boston.data, boston.target\n",
    "\n",
    "# 2. Split data (80% train, 20% test)\n",
    "\n",
    "# 3. Standardize features\n",
    "\n",
    "# 4. Convert to PyTorch tensors\n",
    "# X_train_tensor = ...\n",
    "# y_train_tensor = ...\n",
    "\n",
    "# 5. Define model architecture\n",
    "# class BostonPredictor(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super().__init__()\n",
    "#         # Define layers\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Forward pass\n",
    "\n",
    "# 6. Create model instance\n",
    "# model = BostonPredictor(input_size=X_train.shape[1])\n",
    "\n",
    "# 7. Training loop\n",
    "# ...\n",
    "\n",
    "# 8. Evaluate on test set\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     test_predictions = model(X_test_tensor)\n",
    "#     test_loss = nn.MSELoss()(test_predictions, y_test_tensor)\n",
    "#     print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "# 9. Plot predictions vs actual\n",
    "# plt.scatter(y_test_tensor.numpy(), test_predictions.numpy())\n",
    "# plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "# plt.xlabel('Actual Prices')\n",
    "# plt.ylabel('Predicted Prices')\n",
    "# plt.title('Boston Housing Predictions')\n",
    "# plt.show()\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Challenge Problems\n",
    "\n",
    "### Challenge 1: Debug a Non-Training Model\n",
    "\n",
    "Create a model that SHOULD learn but doesn't (loss stays constant). Include at least 3 common bugs. Then write debugging code to identify each bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== CHALLENGE 1 ===========\n",
    "# Create a buggy model\n",
    "class BuggyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Add bugs here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add bugs here\n",
    "        pass\n",
    "\n",
    "# Debugging function\n",
    "def debug_model(model, X, y):\n",
    "    \"\"\"Identify why model isn't training\"\"\"\n",
    "    print(\"Debugging Model...\")\n",
    "    \n",
    "    # Check 1: Are gradients being computed?\n",
    "    \n",
    "    # Check 2: Are parameters updating?\n",
    "    \n",
    "    # Check 3: Is loss changing?\n",
    "    \n",
    "    # Check 4: Are weights reasonable?\n",
    "    \n",
    "    # Check 5: Is learning rate appropriate?\n",
    "    \n",
    "    print(\"Debugging complete!\")\n",
    "\n",
    "# Create and debug model\n",
    "# model = BuggyModel()\n",
    "# debug_model(model, X, y)\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Create a Learning Rate Finder\n",
    "\n",
    "Implement a learning rate finder that:\n",
    "1. Trains the model with exponentially increasing learning rates\n",
    "2. Plots loss vs learning rate\n",
    "3. Identifies optimal learning rate range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== CHALLENGE 2 ===========\n",
    "def find_learning_rate(model, X, y, min_lr=1e-5, max_lr=1, steps=100):\n",
    "    \"\"\"Find optimal learning rate range\"\"\"\n",
    "    \n",
    "    # Your implementation here\n",
    "    \n",
    "    # Return best learning rate\n",
    "    return best_lr\n",
    "\n",
    "# Usage:\n",
    "# model = YourModel()\n",
    "# optimal_lr = find_learning_rate(model, X_train, y_train)\n",
    "# print(f\"Optimal learning rate: {optimal_lr}\")\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Assessment Questions\n",
    "\n",
    "Answer these questions in markdown cells:\n",
    "\n",
    "### Q1: What's the difference between these two lines?\n",
    "```python\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "x = torch.Tensor([1.0, 2.0, 3.0])\n",
    "```\n",
    "\n",
    "### Q2: When should you use `model.train()` vs `model.eval()`?\n",
    "\n",
    "### Q3: Why do we need both `loss.backward()` and `optimizer.step()`? Can't one function do both?\n",
    "\n",
    "### Q4: What happens if you forget to call `optimizer.zero_grad()` in a training loop?\n",
    "\n",
    "### Q5: How do you move a model to GPU? What common error occurs if you forget something?\n",
    "\n",
    "### Q6: What's the difference between saving `model.state_dict()` and saving the entire model with `torch.save(model, ...)`?\n",
    "\n",
    "### Q7: Why is it important to use `with torch.no_grad():` during inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Progress Tracker\n",
    "\n",
    "Check off exercises as you complete them:\n",
    "\n",
    "- [ ] Exercise 1A: Manual Gradient Verification\n",
    "- [ ] Exercise 1B: Gradient Accumulation\n",
    "- [ ] Exercise 2A: Convert NumPy Network\n",
    "- [ ] Exercise 2B: Test Network\n",
    "- [ ] Exercise 3A: Fix Buggy Training Loop\n",
    "- [ ] Exercise 3B: Write Correct Training Loop\n",
    "- [ ] Exercise 4A: Save/Load Model Weights\n",
    "- [ ] Exercise 4B: Training Checkpoints\n",
    "- [ ] Exercise 5: Boston Housing Prediction\n",
    "- [ ] Challenge 1: Debug Non-Training Model\n",
    "- [ ] Challenge 2: Learning Rate Finder\n",
    "- [ ] Assessment Questions Q1-Q7\n",
    "\n",
    "## üèÜ Completion Certificate\n",
    "\n",
    "Once you complete all exercises, you've mastered:\n",
    "- ‚úÖ PyTorch tensor operations and autograd\n",
    "- ‚úÖ Building neural networks with nn.Module\n",
    "- ‚úÖ Complete training loops\n",
    "- ‚úÖ Model persistence\n",
    "- ‚úÖ Real-world applications\n",
    "- ‚úÖ Debugging skills\n",
    "\n",
    "**You're ready for Lecture 2: Advanced PyTorch Patterns!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
