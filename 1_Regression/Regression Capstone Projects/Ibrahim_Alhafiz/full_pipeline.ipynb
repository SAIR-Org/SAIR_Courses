{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ AirQo PM2.5 Prediction Pipeline\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a production-ready machine learning pipeline to forecast **Particulate Matter (PM2.5)** concentrations across African cities using satellite-derived observations. \n",
    "\n",
    "### Workflow Steps:\n",
    "1.  **Data Loading & Cleaning**: Ingestion and target outlier removal.\n",
    "2.  **EDA**: analyzing distributions and correlations.\n",
    "3.  **Preprocessing**: Custom feature engineering (Time extraction, Outlier clipping).\n",
    "4.  **Modeling**: Training a LightGBM regressor within a Scikit-Learn Pipeline.\n",
    "5.  **Experiment Tracking**: Logging metrics and artifacts using **MLflow**.\n",
    "6.  **Inference**: Generating predictions for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "We start by importing necessary libraries and defining a `Config` class to centralize all hyperparameters and file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import math\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import warnings\n",
    "from typing import List, Literal\n",
    "\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # --- Paths ---\n",
    "    # Update these paths if your data is located elsewhere\n",
    "    BASE_DIR = Path.cwd().parent\n",
    "    DATA_DIR = BASE_DIR / \"data\" / \"raw\"\n",
    "    RAW_DATA_PATH = DATA_DIR / \"Train.csv\"  # Assumes file is in current directory or adjust path\n",
    "    TEST_DATA_PATH = DATA_DIR / \"Test.csv\"\n",
    "    MODEL_DIR = BASE_DIR / \"models\" / \"artifacts\"\n",
    "    MLFLOW_URI = f\"file://{BASE_DIR / 'models' / 'mlruns'}\"\n",
    "    FIGURES_DIR = BASE_DIR / \"reports\" / \"figures\"\n",
    "\n",
    "    # --- Experiment Configs ---\n",
    "    EXPERIMENT_NAME = \"airqo_pm25_prediction\"\n",
    "    \n",
    "    # --- Data Config ---\n",
    "    TARGET = \"pm2_5\"\n",
    "    ID_COL = \"id\"\n",
    "    # Columns to drop (high cardinality or irrelevant for training)\n",
    "    DROP_COLS = [\"id\",\"site_id\", \"site_latitude\", \"site_longitude\", \"city\", \"country\"]\n",
    "    DATE_COL = \"date\"\n",
    "    \n",
    "    # --- Hyperparameters ---\n",
    "    OUTLIER_QUANTILE = 0.02\n",
    "    RANDOM_STATE = 42\n",
    "    MODEL_TYPE = 'lgbm'\n",
    "    TIME_FEATURES = [\"month\", \"week\", \"day_of_week\", \"hour\"]\n",
    "    CORRELATION_THRESH = 0.85\n",
    "    MISSING_THRESH = 0.5\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(Config.MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(Config.FIGURES_DIR, exist_ok=True)\n",
    "print(\"âœ… Configuration loaded and directories created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Cleaning\n",
    "We define functions to load the data and remove extreme outliers from the target variable (`pm2_5`). This helps stabilize the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads CSV data safely.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"âœ… Loaded data from {path}: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ File not found at {path}. Please check your path.\")\n",
    "        return None\n",
    "\n",
    "def clean_target_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Removes rows where target is outside specific quantiles.\"\"\"\n",
    "    upper = df[Config.TARGET].quantile(1 - Config.OUTLIER_QUANTILE)\n",
    "    lower = df[Config.TARGET].quantile(Config.OUTLIER_QUANTILE)\n",
    "    \n",
    "    initial_len = len(df)\n",
    "    df_clean = df[(df[Config.TARGET] > lower) & (df[Config.TARGET] < upper)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"ðŸ§¹ Removed {initial_len - len(df_clean)} target outliers (Boundaries: {lower:.2f} - {upper:.2f}).\")\n",
    "    return df_clean\n",
    "\n",
    "# Load and clean the training data\n",
    "df_train = load_data(Config.RAW_DATA_PATH)\n",
    "if df_train is not None:\n",
    "    df_train_clean = clean_target_outliers(df_train)\n",
    "    df_train_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "Before training, we visualize the target distribution and feature correlations to understand the data landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_regression_plot(y_true, y_pred, saving_dir):\n",
    "    \"\"\"Plots y_true vs y_pred with a regression fit line and RMSE.\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5, color='blue', label='Predictions')\n",
    "    \n",
    "    # Calculate regression line fit (y = mx + c)\n",
    "    m, c = np.polyfit(y_true, y_pred, 1)\n",
    "    plt.plot(y_true, m*y_true + c, color='red', label=f'Fit Line (m={m:.2f})')\n",
    "    \n",
    "    plt.title(f'Actual vs Predicted (RMSE: {rmse:.4f})')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Save the plot\n",
    "    save_path = os.path.join(saving_dir, \"regression_fit.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close() # Close to free up memory\n",
    "    print(f\"Regression plot saved to: {save_path}\")\n",
    "\n",
    "def plot_feature_importance(pipeline, model_name):\n",
    "    \"\"\"Extracts and saves feature importance using automatic name discovery.\"\"\"\n",
    "    \n",
    "    # 1. Get the names from the preprocessing steps\n",
    "    # We slice [:-1] to get everything EXCEPT the regressor\n",
    "    preprocessor_part = pipeline[:-1]\n",
    "    feature_names = preprocessor_part.get_feature_names_out()\n",
    "    \n",
    "    # 2. Get the model (the last step)\n",
    "    model = pipeline.steps[-1][1]\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # 3. Create DataFrame\n",
    "        feat_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values(by='Importance', ascending=False).head(10)\n",
    "\n",
    "        # 4. Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=feat_df, x='Importance', y='Feature', hue='Feature')\n",
    "        plt.title(f'Top 10 Features - {model_name}')\n",
    "        # plt.yticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Model {model_name} does not support feature importances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution(data, target):\n",
    "    from scipy.stats import norm\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data[target], kde=True, stat=\"density\", color=\"skyblue\", alpha=0.6)\n",
    "    \n",
    "    # Fit a normal distribution\n",
    "    (mu, sigma) = norm.fit(data[target])\n",
    "    print(f'Target stats: mu = {mu:.2f} and sigma = {sigma:.2f}')\n",
    "    \n",
    "    plt.title(f'{target} Distribution vs Normal Distribution')\n",
    "    plt.xlabel('PM 2.5 Concentration')\n",
    "    plt.show()\n",
    "\n",
    "if df_train is not None:\n",
    "    plot_target_distribution(df_train_clean, Config.TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(data):\n",
    "    # Select only numeric columns\n",
    "    num_data = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr = num_data.corr()\n",
    "    sns.heatmap(corr, cmap='coolwarm', annot=False, fmt=\".2f\")\n",
    "    plt.title(\"Feature Correlation Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "if df_train is not None:\n",
    "    plot_correlations(df_train_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Transformers & Pipeline\n",
    "We implement modular Scikit-Learn transformers to handle:\n",
    "1.  **Time Extraction**: Converting date strings into features like `month`, `hour`, etc.\n",
    "2.  **Outlier Handling**: Clipping feature values to reduce noise.\n",
    "\n",
    "These are integrated into a `Pipeline` to ensure no data leakage occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1 Time Feature Extractor ---\n",
    "class TimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts temporal features and handles feature name propagation.\"\"\"\n",
    "    def __init__(self, date_col: str, features_to_extract: List[str] = None):\n",
    "        self.date_col = date_col\n",
    "        self.features_to_extract = features_to_extract or [\"month\", \"hour\"]\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        self.feature_names_in_ = np.array(X.columns.tolist())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_out = X.copy()\n",
    "        X_out[self.date_col] = pd.to_datetime(X_out[self.date_col])\n",
    "        \n",
    "        if \"month\" in self.features_to_extract:\n",
    "            X_out[f\"{self.date_col}_month\"] = X_out[self.date_col].dt.month\n",
    "        if \"hour\" in self.features_to_extract:\n",
    "            X_out[f\"{self.date_col}_hour\"] = X_out[self.date_col].dt.hour\n",
    "        if \"week\" in self.features_to_extract:\n",
    "            X_out[f\"{self.date_col}_week\"] = X_out[self.date_col].dt.isocalendar().week.astype(int)\n",
    "        if \"day_of_week\" in self.features_to_extract:\n",
    "            X_out[f\"{self.date_col}_dow\"] = X_out[self.date_col].dt.dayofweek\n",
    "        \n",
    "        return X_out.drop(columns=[self.date_col])\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features is None: input_features = self.feature_names_in_\n",
    "        features = [f for f in input_features if f != self.date_col]\n",
    "        new_feats = [f\"{self.date_col}_{t}\" for t in self.features_to_extract if t in [\"month\", \"hour\", \"week\", \"day_of_week\"]]\n",
    "        return np.array(features + new_feats)\n",
    "\n",
    "\n",
    "# --- 4.2 High Missing Dropper ---\n",
    "class HighMissingDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops columns with missing percentage above a threshold.\"\"\"\n",
    "    def __init__(self, threshold: float = 0.5):\n",
    "        self.threshold = threshold\n",
    "        self.drop_cols_ = []\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        # Calculate missing percentage per column\n",
    "        missing_frac = X.isnull().mean()\n",
    "        self.drop_cols_ = missing_frac[missing_frac > self.threshold].index.tolist()\n",
    "        print(f\"\\nðŸ—‘ï¸ HighMissingDropper: Will drop {len(self.drop_cols_)} columns > {self.threshold*100}% missing.\")\n",
    "        self.feature_names_in_ = np.array(X.columns.tolist())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Drop identified columns\n",
    "        return X.drop(columns=self.drop_cols_, errors='ignore')\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features is None: input_features = self.feature_names_in_\n",
    "        return np.array([f for f in input_features if f not in self.drop_cols_])\n",
    "\n",
    "\n",
    "# --- 4.3 Correlated Feature Aggregator ---\n",
    "class CorrelatedFeatureAggregator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Automatically groups correlated features (> threshold) using a graph-based approach,\n",
    "    replaces them with their mean, and drops the originals.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold: float = 0.85):\n",
    "        self.threshold = threshold\n",
    "        self.groups_ = {} # Maps new_name -> [list_of_cols]\n",
    "        self.drop_cols_ = []\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        self.feature_names_in_ = np.array(X.columns.tolist())\n",
    "        \n",
    "        # Only consider numeric columns for correlation\n",
    "        num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if len(num_cols) < 2:\n",
    "            return self\n",
    "\n",
    "        # 1. Compute Correlation Matrix\n",
    "        corr_matrix = X[num_cols].corr().abs()\n",
    "        \n",
    "        # 2. Find Connected Components (Groups)\n",
    "        # We treat features as nodes and high correlation as edges.\n",
    "        processed = set()\n",
    "        group_id = 1\n",
    "        \n",
    "        for col in num_cols:\n",
    "            if col in processed:\n",
    "                continue\n",
    "            \n",
    "            # Find all features connected to 'col' (including itself)\n",
    "            group = [col]\n",
    "            stack = [col]\n",
    "            processed.add(col)\n",
    "            \n",
    "            while stack:\n",
    "                current = stack.pop()\n",
    "                # Get neighbors with corr > threshold\n",
    "                neighbors = corr_matrix[current][corr_matrix[current] > self.threshold].index.tolist()\n",
    "                for neighbor in neighbors:\n",
    "                    if neighbor not in processed:\n",
    "                        processed.add(neighbor)\n",
    "                        stack.append(neighbor)\n",
    "                        group.append(neighbor)\n",
    "            \n",
    "            # If group has more than 1 feature, save it\n",
    "            if len(group) > 1:\n",
    "                new_name = f\"agg_corr_group_{group_id}\"\n",
    "                self.groups_[new_name] = group\n",
    "                self.drop_cols_.extend(group)\n",
    "                group_id += 1\n",
    "                \n",
    "        print(f\"ðŸ”— CorrelatedFeatureAggregator: Found {len(self.groups_)} groups to aggregate (Threshold: {self.threshold}).\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_out = X.copy()\n",
    "        \n",
    "        # Create aggregated mean columns\n",
    "        for new_col, components in self.groups_.items():\n",
    "            # Compute mean row-wise\n",
    "            X_out[new_col] = X_out[components].mean(axis=1)\n",
    "            \n",
    "        # Drop original columns\n",
    "        return X_out.drop(columns=self.drop_cols_, errors='ignore')\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features is None: input_features = self.feature_names_in_\n",
    "        # Remove dropped, add new\n",
    "        kept = [f for f in input_features if f not in self.drop_cols_]\n",
    "        new = list(self.groups_.keys())\n",
    "        return np.array(kept + new)\n",
    "\n",
    "\n",
    "# --- 4.4 Outlier Handler ---\n",
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Clips outliers using IQR method.\"\"\"\n",
    "    def __init__(self, factor: float = 1.5):\n",
    "        self.factor = factor\n",
    "        self.lower_bounds_ = {}\n",
    "        self.upper_bounds_ = {}\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        self.feature_names_in_ = np.array(X.columns.tolist())\n",
    "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            Q1 = X[col].quantile(0.25)\n",
    "            Q3 = X[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            self.lower_bounds_[col] = Q1 - (self.factor * IQR)\n",
    "            self.upper_bounds_[col] = Q3 + (self.factor * IQR)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_out = X.copy()\n",
    "        for col, lower in self.lower_bounds_.items():\n",
    "            if col in X_out.columns:\n",
    "                upper = self.upper_bounds_[col]\n",
    "                X_out[col] = np.clip(X_out[col], lower, upper)\n",
    "        return X_out\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names_in_\n",
    "\n",
    "class SmartColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer that safely drops or keeps columns based on a strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    columns : List[str]\n",
    "        The list of columns to act upon.\n",
    "    strategy : 'drop' or 'keep', default='drop'\n",
    "        'drop': Removes the specified columns from the dataframe.\n",
    "        'keep': Keeps ONLY the specified columns, dropping everything else.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns: List[str], strategy: Literal['drop', 'keep'] = 'drop'):\n",
    "        self.columns = columns\n",
    "        self.strategy = strategy\n",
    "        self.feature_names_in_ = None\n",
    "        self.final_columns_ = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        self.feature_names_in_ = np.array(X.columns.tolist())\n",
    "        \n",
    "        if self.strategy == 'drop':\n",
    "            # Check for columns requested to drop that aren't there\n",
    "            missing_cols = [c for c in self.columns if c not in X.columns]\n",
    "            if missing_cols:\n",
    "                warnings.warn(\n",
    "                    f\"âš ï¸ SmartColumnDropper (drop): Columns not found to drop: {missing_cols}\"\n",
    "                )\n",
    "            self.final_columns_ = [c for c in X.columns if c not in self.columns]\n",
    "            \n",
    "        elif self.strategy == 'keep':\n",
    "            # Check for columns requested to keep that aren't there\n",
    "            missing_cols = [c for c in self.columns if c not in X.columns]\n",
    "            if missing_cols:\n",
    "                warnings.warn(\n",
    "                    f\"âš ï¸ SmartColumnDropper (keep): Columns requested to keep but missing from input: {missing_cols}\"\n",
    "                )\n",
    "            # We can only keep what actually exists\n",
    "            self.final_columns_ = [c for c in self.columns if c in X.columns]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Strategy must be either 'drop' or 'keep'.\")\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Use the columns identified during fit\n",
    "        return X[self.final_columns_].copy()\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"\n",
    "        Returns the names of the features that remain after the transformation.\n",
    "        \"\"\"\n",
    "        return np.array(self.final_columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.5 Pipeline Construction ---\n",
    "\n",
    "def build_pipeline(cat_cols: list, num_cols: list) -> Pipeline:\n",
    "    \"\"\"Constructs the full processing and modeling pipeline.\"\"\"\n",
    "    \n",
    "    # Model Registry\n",
    "    MODELS = {\n",
    "        \"lgbm\": LGBMRegressor(verbose=-1, random_state=Config.RANDOM_STATE),\n",
    "        \"xgboost\": XGBRegressor(random_state=Config.RANDOM_STATE),\n",
    "        \"rf\": RandomForestRegressor(random_state=Config.RANDOM_STATE)\n",
    "    }\n",
    "\n",
    "    # 1. Feature Engineering (Applied sequentially)\n",
    "    # Note: These transformers handle dataframe input/output\n",
    "    feature_engineering = Pipeline([\n",
    "        (\"column_filterer\", SmartColumnDropper(Config.DROP_COLS)),\n",
    "        ('corr_aggregator', CorrelatedFeatureAggregator(threshold=Config.CORRELATION_THRESH)),\n",
    "        ('missing_dropper', HighMissingDropper(threshold=Config.MISSING_THRESH)),\n",
    "        ('time_extractor', TimeFeatureExtractor(date_col=Config.DATE_COL, features_to_extract=Config.TIME_FEATURES)),\n",
    "        ('outlier_clipper', OutlierHandler(factor=1.5)),\n",
    "    ])\n",
    "    \n",
    "    # 2. Column Preprocessing (Handling missing values & scaling)\n",
    "    # NOTE: Since feature_engineering changes columns dynamically, we cannot rely on \n",
    "    # fixed lists 'num_cols' passed at the start. \n",
    "    # We must use 'make_column_selector' to select columns dynamically after feature engineering.\n",
    "    from sklearn.compose import make_column_selector\n",
    "\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', RobustScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, make_column_selector(dtype_include=np.number)),\n",
    "        ('cat', categorical_transformer, make_column_selector(dtype_include=object))\n",
    "    ])\n",
    "    \n",
    "    # 3. Final Assembly\n",
    "    pipeline = Pipeline([\n",
    "        ('feature_eng', feature_engineering),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', MODELS[Config.MODEL_TYPE])\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training & Validation\n",
    "We use a custom validation strategy based on cities to ensure the model generalizes well to new locations. We also use **MLflow** to track the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1 Helper Functions ---\n",
    "\n",
    "def get_train_val_folds(X):\n",
    "    \"\"\"Generates folds based on city combinations to test spatial generalization.\"\"\"\n",
    "    def get_all_combinations(input_list):\n",
    "        all_combinations = []\n",
    "        for r in range(1, len(input_list)):\n",
    "            all_combinations.extend(list(itertools.combinations(input_list, r)))\n",
    "        return all_combinations\n",
    "\n",
    "    unique_cities = X['city'].unique()\n",
    "    folds = {}\n",
    "    \n",
    "    # Simplified for notebook demo: Taking limited combinations to save time\n",
    "    # In production, use the full list from ingest.py\n",
    "    combos = get_all_combinations(unique_cities.tolist())\n",
    "    \n",
    "    for i, cities in enumerate(combos):\n",
    "        train_mask = X['city'].isin(cities)\n",
    "        # Ensure meaningful split size (e.g. valid set is smaller than train)\n",
    "        if len(X[train_mask]) < len(X[~train_mask]):\n",
    "            continue\n",
    "            \n",
    "        folds[i] = (X[train_mask].index.values, X[~train_mask].index.values)\n",
    "        \n",
    "    print(f\"Created {len(folds)} validation folds based on city splits.\")\n",
    "    return folds\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"r2\": r2_score(y_true, y_pred),\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def plot_results(y_true, y_pred):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.3, color='purple')\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual PM2.5')\n",
    "    plt.ylabel('Predicted PM2.5')\n",
    "    plt.title('Actual vs Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.2 Main Training Workflow ---\n",
    "\n",
    "def train_workflow(df):\n",
    "    # Setup MLflow\n",
    "    mlflow.set_tracking_uri(Config.MLFLOW_URI)\n",
    "    mlflow.set_experiment(Config.EXPERIMENT_NAME)\n",
    "    \n",
    "    # Prepare Data\n",
    "    X = df.drop(columns=[Config.TARGET])\n",
    "    y = df[Config.TARGET]\n",
    "    \n",
    "    # Define Folds\n",
    "    folds = get_train_val_folds(df)\n",
    "    \n",
    "    # Define Column types for Pipeline\n",
    "    # Exclude Drop Cols & Date Col (handled by transformer)\n",
    "    features = [c for c in X.columns if c not in Config.DROP_COLS and c != Config.DATE_COL]\n",
    "    cat_cols = X[features].select_dtypes(exclude=['number']).columns.tolist()\n",
    "    num_cols = X[features].select_dtypes(include=['number']).columns.tolist()\n",
    "    # Note: We pass 'num_cols' here, but our custom transformer adds time features.\n",
    "    # In a stricter pipeline, we would use sklearn's make_column_selector.\n",
    "    # For this notebook, we assume the numeric transformer handles the raw numeric cols.\n",
    "    pipeline = build_pipeline(cat_cols, num_cols)\n",
    "    \n",
    "    print(\"\\nðŸš€ Starting Training Run...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"notebook_run\"):\n",
    "        \n",
    "        # Cross Validation Loop\n",
    "        all_metrics = []\n",
    "        preds = np.zeros(len(y))\n",
    "        preds_counts = np.zeros(len(y))\n",
    "        \n",
    "        for fold_id, (train_idx, val_idx) in folds.items():\n",
    "            X_train, X_val = X.iloc[train_idx,:], X.iloc[val_idx,:]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_val)\n",
    "            \n",
    "            metrics = evaluate_metrics(y_val, y_pred)\n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "            # Accumulate for OOF predictions\n",
    "            preds[val_idx] += y_pred\n",
    "            preds_counts[val_idx] += 1\n",
    "            \n",
    "        # Aggregate Metrics\n",
    "        avg_metrics = pd.DataFrame(all_metrics).mean().to_dict()\n",
    "        print(f\"âœ… Cross-Validation Metrics: {avg_metrics}\")\n",
    "        \n",
    "        # Log to MLflow\n",
    "        mlflow.log_params(pipeline.named_steps['regressor'].get_params())\n",
    "        mlflow.log_metrics(avg_metrics)\n",
    "        \n",
    "        # Train on Full Dataset and Save\n",
    "        pipeline.fit(X, y)\n",
    "        joblib.dump(pipeline, Config.MODEL_DIR / \"final_pipeline.pkl\")\n",
    "        mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "            \n",
    "    return pipeline\n",
    "\n",
    "if df_train is not None:\n",
    "    trained_pipeline = train_workflow(df_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(trained_pipeline, Config.MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference\n",
    "Finally, we load the test dataset and generate predictions using the trained pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(input_path, pipeline):\n",
    "    try:\n",
    "        df_test = pd.read_csv(input_path)\n",
    "        print(f\"ðŸ” Loaded test data: {df_test.shape}\")\n",
    "        \n",
    "        if Config.ID_COL not in df_test.columns:\n",
    "             raise ValueError(f\"Missing ID column: {Config.ID_COL}\")\n",
    "        \n",
    "        predictions = pipeline.predict(df_test)\n",
    "        \n",
    "        submission = pd.DataFrame({\n",
    "            Config.ID_COL: df_test[Config.ID_COL],\n",
    "            Config.TARGET: predictions\n",
    "        })\n",
    "        \n",
    "        output_path = Config.DATA_DIR / \"outputs\" / \"submission.csv\"\n",
    "        os.makedirs(output_path.parent, exist_ok=True)\n",
    "        submission.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"âœ… Submission saved to {output_path}\")\n",
    "        return submission.head()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"âš ï¸ Test file not found. Skipping inference.\")\n",
    "        return None\n",
    "\n",
    "if df_train is not None:\n",
    "    predict_and_save(Config.TEST_DATA_PATH, trained_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have successfully converted a multi-script project into a single, cohesive notebook. This pipeline handles everything from raw data ingestion to MLflow tracking, ensuring a robust and reproducible machine learning workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
